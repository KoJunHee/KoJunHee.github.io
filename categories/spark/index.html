<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Category: Spark - Always Learning</title><meta property="og:type" content="blog"><meta property="og:title" content="Always Learning"><meta property="og:url" content="https://kojunhee.github.io/"><meta property="og:site_name" content="Always Learning"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://kojunhee.github.io/img/og_image.png"><meta property="article:author" content="junhee.ko"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://kojunhee.github.io"},"headline":"Always Learning","image":["https://kojunhee.github.io/img/og_image.png"],"author":{"@type":"Person","name":"junhee.ko"},"description":null}</script><link rel="icon" href="/img/favicon.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/default.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><script data-ad-client="ca-pub-6880109808178384" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js" async></script></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Always Learning" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a></div><div class="navbar-end"><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li class="is-active"><a href="#" aria-current="page">Spark</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2019-08-06T15:00:00.000Z" title="2019-08-06T15:00:00.000Z">2019-08-07</time><span class="level-item"><a class="link-muted" href="/categories/spark/">Spark</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/08/07/spark-sql/">[스파크2 프로그래밍] 5장_스파크SQL 과 데이터프레임,데이터셋</a></h1><div class="content"><p>RDD 의 장점은, </p>
<ul>
<li>분산환경에서 메모리 기반으로 빠르고 안정적으로 동작하는 프로그램을 만들 수 있다는 것과, </li>
<li>RDD 가 제공하는 풍부한 데이터 처리 연산입니다. ( &lt;-&gt; 맵과 리듀스로만 문제를 해결 ) </li>
</ul>
<p>RDD 의 단점은,</p>
<ul>
<li>스키마를 표현할 수 없다는 것입니다. </li>
</ul>
<p>스파크 SQL 은 RDD 의 이 단점을 보완할 수 있도록 또 다른 유형의 데이터 모델과 API를 제공하는 스파크 모듈입니다. 스파크 SQL 에서 데이터를 다루는 방법은 SQL을 사용하는 것과 데이터셋 API 를 사용하는 방법이 있습니다. </p>
<p>데이터셋은 스파크 1.6 버젼에서 처음 소개된 것으로 자바와 스칼라 언어에서만 사용할 수 있었고, 그 이전에는 데이터프레임이라는 클래스를 구현 언어와 상관없이 사용하고 있었습니다. <strong>스파크 2.0 부터 데이터프레임 클래스가 데이터셋 클래스로 통합</strong>되면서 Type Alias 라는 독특한 기능을 가진 스칼라 언어에서만 기존과 같은 데이터 프레임을 사용할 수 있고 해당 기능이 없는 자바에서는 데이터셋 클래스만을 사용할 수 있게 됐습니다. </p>
<p>스칼라의 데이터프레임은 “type DataFrame = Dataset[Row]” 와 같이 정의돼 있는데 바로 이 구문이 Type Alias 에 해당하는 부분으로, <strong>Dataset 의 타입 파라미터가 Row 인 경우를 DataFrame</strong> 이라는 이름으로도 사용하겠다는 의미입니다.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ds = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>).toDS</span><br><span class="line">ds.show</span><br><span class="line">ds.printScheme</span><br></pre></td></tr></table></figure>

<h3 id="5-1-데이터셋"><a href="#5-1-데이터셋" class="headerlink" title="5.1 데이터셋"></a>5.1 데이터셋</h3><p>데이터셋 이전에는 데이터프레임이라는 API 를 사용했습니다. 가장 큰 특징은, 기존 RDD 와는 다른 형태를 가진 <strong>SQL 과 유사한 방식의 연산을 제공</strong>했다는 점입니다. 예를 들어,</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.map(v=&gt;v+<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>와 같은 map() 연산을 사용한 것에 반해, 동일한 요소로 구성된 데이터프레임에서는 </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(df(<span class="string">"value"</span>)+<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>와 같은 방법을 사용했습니다. df는 데이터프레임을 df(“value”) 는 데이터프레임이 가지고 있는 “value” 라는 이름의 칼럼을 의미하는 것으로, 마치 SQL 의 SELECT 문과 유사한 형식으로 데이터를 조회하는 방법입니다.</p>
<p>이러한 데이터 프레임의 장점은, </p>
<ul>
<li>풍부한 API 와 옵티마이저를 기반으로 한 높은 성능으로 복잡한 데이터 처리를 수월하게 수행할 수 있습니다.</li>
</ul>
<p>하지만,</p>
<ul>
<li>처리해야하는 작업의 특성에 따라 RDD 를 사용하는것에 비해 복잡한 코드를 작성하거나 컴파일 타임 오류 체크 기능을 사용할 수 없는 단점이 있습니다.</li>
</ul>
<p>데이터셋은 데이터프레임이 제공하던 <strong>성능 최적화 같은 장점을 유지하면서 RDD 에서만 가능했던 컴파일 타임 오류 체크 등의 기능을 사용</strong>할 수 있게 됐습니다. </p>
<h3 id="5-2-연산의-종류와-주요-API"><a href="#5-2-연산의-종류와-주요-API" class="headerlink" title="5.2 연산의 종류와 주요 API"></a>5.2 연산의 종류와 주요 API</h3><p>데이터셋이 제공하는 연산은 RDD 와 마찬가지로 두 종류로 분류합니다.</p>
<ul>
<li>트렌스포메이션 <ul>
<li>새로운 데이터셋을 생성하는 연산. (액션 연산이 호출될 때까지 수행되지 않습니다)</li>
</ul>
</li>
<li>액션연산<ul>
<li>실제 데이터 처리를 수행하고 결과를 생성하는 연산 </li>
</ul>
</li>
</ul>
<p>트랜스포메이션 연산은 데이터 타입을 처리하는 방법에 따라 두 가지로 나뉩니다.</p>
<ul>
<li>타입 연산</li>
<li>비타입 연산</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> data = <span class="number">1</span> to <span class="number">100</span> toList</span><br><span class="line">scala&gt; <span class="keyword">val</span> ds = data.toDS <span class="comment">//데이터셋 생성</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> result = ds.map(_+<span class="number">1</span>) <span class="comment">// == ds.select(col("value") + 1)</span></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ds.select(col(<span class="string">"value"</span>) + <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>이것은 데이터셋을 데이터베이스의 테이블과 유사하게 처리한 것으로, 마치 value 라는 칼럼을 가진 ds 라는 테이블에서 value 칼럼에 +1 을 한 결과를 조회하는 것과 유사한 방법입니다. 이 때, col(“value”) 라는 부분이 칼럼을 나타내며, 이 타입이 원래 데이터 타입인 정수가 아니라 org.apache.spark.sql.Column 타입입니다.</p>
<p>데이터셋에는 SQL 과 유사한 방식의 데이터 처리를 위해 데이터베이스의 Row 와 Column 에 해당하는 타입을 정의하고 있으며, 실제 데이터가 어떤 타입이든지 로우와 칼럼 타입으로 감싸러 처리할 수 있는 구조입니다.</p>
<p><strong>비타입 연산이란, 데이터를 처리할 때 데이터 본래의 타입이 아닌 org.apache.spark.sql.Column 과 org.apache.spark.sql.Row  타입의 객체로 감싸서 처리하는 연산</strong>입니다.</p>
<p>데이터프레임은 org.apache.spark.sql.Row  타입의 요소로 구성된 데이터셋을 가리키는 용어입니다. 모든 데이터를 Row 타입으로 변환해서 생성된 데이터셋을 가리켜 데이터프레임이라는 별칭으로 부르기 대문에 이런 연산들을 데이터 프레임 연산이라고 부르기도 합니다.</p>
<h3 id="5-3-코드-작성-절차-및-단어-수-세게-예제"><a href="#5-3-코드-작성-절차-및-단어-수-세게-예제" class="headerlink" title="5.3 코드 작성 절차 및 단어 수 세게 예제"></a>5.3 코드 작성 절차 및 단어 수 세게 예제</h3><p>단계별 스파크 SQL 코드 작성 방법입니다.</p>
<ol>
<li><p>스파크 세션 생성 (RDD 를 생성하기 위해 SparkContext가 필요한 것처럼, 데이터 프레임을 생성하기 위해 필요)</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SparkSession spark = SparkSession</span><br><span class="line">  .builder()</span><br><span class="line">  .appName(<span class="string">"jko"</span>) <span class="comment">//application name</span></span><br><span class="line">  .master(<span class="string">"local[*]"</span>) <span class="comment">//master info</span></span><br><span class="line">  .getOrCreate();</span><br></pre></td></tr></table></figure>
</li>
<li><p>스파크세션으로부터 데이터셋 or 데이터프레임 생성</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">String source = <span class="string">"file://&lt;spark_home_dir&gt;/README.md"</span>;</span><br><span class="line">Dataset&lt;Row&gt; df = spark.read().test(source); <span class="comment">//read() 는 DataFrameReader 인스턴스 리턴</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>생성된 데이터셋 또는 데이터프레임을 이용해 데이터 처리</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ds = df.as[(<span class="type">String</span>)] <span class="comment">// 데이터 프레임을 데이터 셋으로 변환. Row 타입 요소 -&gt; 원래 데이터 타입</span></span><br><span class="line"><span class="keyword">val</span> wordDF = ds.flatMap(_.split(<span class="string">" "</span>)) <span class="comment">// 문장을 각 단어로 분리</span></span><br><span class="line"><span class="keyword">val</span> result = wordDF.groupByKey(v =&gt; v).count <span class="comment">// 같은 단어끼리 그룹 생성하고 그룹별 요소 개수 count</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>처리된 결과 데이터를 외부 저장소에 저장</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result.write.test(<span class="string">"&lt;저장경로&gt;"</span>) <span class="comment">// write() 는 DataFrameWriter 를 리턴</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>스파크 세션 종료</p>
</li>
</ol>
<h3 id="5-4-스파크세션"><a href="#5-4-스파크세션" class="headerlink" title="5.4 스파크세션"></a>5.4 스파크세션</h3><p>데이터프레임 또는 데이터셋을 생성하거나 사용자 정의 함수를 등록하기 위한 목적으로 사용됩니다. 스파크 SQL 2.0 부터 사용된 것으로, 기존에는 SQLContext 와 HiveContext를 사용했습니다.</p>
<p>기존 SQLContext 는 스파크세션과 유사하게 데이터 프레임을 생성하고 사용자 정의 함수를 등록하는 기능을 수행했는데, 아파치  하이브에서 제공하는 <strong>HiveQL 을 사용하거나 기존 하이브 서버와 연동하기 위해서는 SQLContext 의 하위 클래스인 HiveContext 를 사용해야</strong>했습니다.</p>
<p>이 두 클래스를 합친 <strong>스파크세션 클래스를 정의하면서 스파크 세션 하나만으로 하이브 지원까지 가능</strong>합니다. </p>
<p>하이브 서버와 연동해서 사용하고 싶으면, 스파크의 conf 디렉토리에 hive-site.xml, core-site.xml., hfs-site.xml 파일을 생성해야 합니다. 하이브 및 하둡에서 사용하는 설정파일입니다.</p>
<h3 id="5-5-데이터프레임-로우-칼럼"><a href="#5-5-데이터프레임-로우-칼럼" class="headerlink" title="5.5 데이터프레임, 로우, 칼럼"></a>5.5 데이터프레임, 로우, 칼럼</h3><p><strong>데이터프레임은 org.apache.spark.sql.Row 타입의 요소를 가진 데이터셋</strong>을 가리키는 별칭으로 사용되고 이습니다. R 의 데이터프레임이나 데이터베이스의 테이블과 비슷한 행과 열의 구조를 가지고 있어서, 데이터프레임에 포함된 데이터는 SQL 문을 사용하거나 데이터프레임이 제공하는 프로그래밍 API 를 이용해 처리 가능합니다.</p>
<p>데이터프레임을 사용하는 궁긍적인 목적은 RDD 와 같이 분산 데이터를 저장하고 처리하기 위한것이지만 RDD 가 데이터의 값을 다루는데 초점을 맞추고 있었다면 <strong>데이터프레임은 데이터 값뿐만 아니라 제공된 스키마 정보를 이용해 RDD 를 통해서는 얻기 힘든 다양한 성능 최적화 기능을 제공</strong>하기 때문입니다.</p>
<h4 id="5-5-1-데이터프레임-생성"><a href="#5-5-1-데이터프레임-생성" class="headerlink" title="5.5.1 데이터프레임 생성"></a>5.5.1 데이터프레임 생성</h4><p>스파크 세션을 이용해 생성합니다. </p>
<p>생성 방법은 <strong>파일이나 데이터베이스와 같은 스파크 외부의 데이터소스에 저장된 데이터를 이용해 생성할 수도 있고 이미 생성돼 있는 다른 RDD 나 데이터프레임에 변환 연산을 적용해 새로운 데이터프레임을 생성</strong>할 수도 있습니다.</p>
<p>다음은 생성 방법 입니다.</p>
<ul>
<li>외부데이터로부터</li>
<li>기존 RDD 및 로컬 컬렉션으로부터</li>
</ul>
<h5 id="5-5-1-1-외부-데이터소스로부터-데이터프레임-생성"><a href="#5-5-1-1-외부-데이터소스로부터-데이터프레임-생성" class="headerlink" title="5.5.1.1 외부 데이터소스로부터 데이터프레임 생성"></a>5.5.1.1 외부 데이터소스로부터 데이터프레임 생성</h5><p>파일이나 데이터베이스 같은 외부저장소에서 데이터를 읽어서 데이터프레임을 생성할 때 가장 손쉽게 처리할 수 있는 방법은 스파크 세션이 제공하는 read() 메서드를 사용하는 것입니다. read() 메서드는 DateFrameReader 인스턴스를 생성하는데, 이를 이용해 다양한 유형의 데이터를 일고 데이터프레임을 생성할 수 있습니다.</p>
<h5 id="5-5-1-2-기존-RDD-및-로컬-컬력센으로부터-데이터-프레임-생성"><a href="#5-5-1-2-기존-RDD-및-로컬-컬력센으로부터-데이터-프레임-생성" class="headerlink" title="5.5.1.2 기존 RDD 및 로컬 컬력센으로부터 데이터 프레임 생성"></a>5.5.1.2 기존 RDD 및 로컬 컬력센으로부터 데이터 프레임 생성</h5><p>스파크 SQL 은 스키마를 지정하는 두 가지 방법을 제공합니다. </p>
<ul>
<li>리플렉션 API 를 이용해 데이터의 스키마 정보를 자동으로 추론하는 방법<ul>
<li>스키마의 정의 위한 별도의 추가 코드가 필요 없어서 간결한 코드를 작성할 수 있다는 장점</li>
</ul>
</li>
<li>개발자가 직접 스키마 정보를 코드로 작성해서 지정<ul>
<li>스키마 추론을 위한 부가적인 연산을 줄이고 스키마 정보를 원하는 대로 커스터마이징해서 사용하고자 한다면 이 방법을 사용</li>
</ul>
</li>
</ul>
<h6 id="5-5-1-2-1-리플렉션을-통한-데이터프레임-생성"><a href="#5-5-1-2-1-리플렉션을-통한-데이터프레임-생성" class="headerlink" title="5.5.1.2.1 리플렉션을 통한 데이터프레임 생성"></a>5.5.1.2.1 리플렉션을 통한 데이터프레임 생성</h6><p><strong>데이터 프레임 내부의 데이터는 동일한 수의 칼럼 정보를 포함하고 있는 로우의 집합</strong>입니다. 그래서 RDD 를 비롯해 로우와 칼럼 형태로 만들 수 있는 컬렉현 객체만 있다면 이를 이용해 새로운 데이터 프레임을 만들 수 있습니다.</p>
<p>다음은 RDD 가 아닌 일반 리스트를 이용해 데이터 프레임을 생성하는 예제입니다.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> calss Person implements Serializable&#123;</span><br><span class="line">  <span class="keyword">private</span> String name;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line">  <span class="keyword">private</span> String job;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Person row1 = <span class="keyword">new</span> Person(<span class="string">"junhee"</span>, <span class="number">6</span>, <span class="string">"student"</span>);</span><br><span class="line">Person row2 = <span class="keyword">new</span> Person(<span class="string">"bill"</span>, <span class="number">12</span>, <span class="string">"god"</span>);</span><br><span class="line">Person row3 = <span class="keyword">new</span> Person(<span class="string">"jesi"</span>, <span class="number">14</span>, <span class="string">"mom"</span>);</span><br><span class="line">Person row4 = <span class="keyword">new</span> Person(<span class="string">"mina"</span>, <span class="number">41</span>, <span class="string">"chef"</span>);</span><br><span class="line"></span><br><span class="line">List&lt;Person&gt; data = Arrays.asList(row1, row2, row3, row4);		</span><br><span class="line">Dataset&lt;Row&gt; df4 = spark.createDataFrame(data, Person<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">df4.show();</span><br></pre></td></tr></table></figure>

<p><strong>특별한 스키마 정의를 추가히지 않아도 컬렉션에 포함된 오브젝트의 속성값으로부터 알아서 스키마 정보를 추출하고 데이터 프레임을 만드는 방법을 리플렉션을 이용한 데이터 프레임 생성 방법</strong>이라고 합니다. </p>
<p>다음은 RDD 로부터 데이터프레임을 생성하는 예제입니다.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> calss Person implements Serializable&#123;</span><br><span class="line">  <span class="keyword">private</span> String name;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line">  <span class="keyword">private</span> String job;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Person row1 = <span class="keyword">new</span> Person(<span class="string">"junhee"</span>, <span class="number">6</span>, <span class="string">"student"</span>);</span><br><span class="line">Person row2 = <span class="keyword">new</span> Person(<span class="string">"bill"</span>, <span class="number">12</span>, <span class="string">"god"</span>);</span><br><span class="line">Person row3 = <span class="keyword">new</span> Person(<span class="string">"jesi"</span>, <span class="number">14</span>, <span class="string">"mom"</span>);</span><br><span class="line">Person row4 = <span class="keyword">new</span> Person(<span class="string">"mina"</span>, <span class="number">41</span>, <span class="string">"chef"</span>);</span><br><span class="line"></span><br><span class="line">List&lt;Person&gt; data = Arrays.asList(row1, row2, row3, row4);	</span><br><span class="line"></span><br><span class="line">JavaRDD&lt;Person&gt; rdd = sc.parallelize(data);</span><br><span class="line">Dataset&lt;Row&gt; df5 = spark.createDataFrame(rdd, Person<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure>

<h6 id="5-5-1-2-2-명시적-타입-지정을-통한-데이터-프레임-생성"><a href="#5-5-1-2-2-명시적-타입-지정을-통한-데이터-프레임-생성" class="headerlink" title="5.5.1.2.2 명시적 타입 지정을 통한 데이터 프레임 생성"></a>5.5.1.2.2 명시적 타입 지정을 통한 데이터 프레임 생성</h6><p>리플렉션 방식은 스키마를 정의하지 않다는 점이 편리하지만, 데이터프레임 생성을 위한 케이스 클래스를 따로 정의해야하는 불편함이 있고 상황에 따라 원하는 대로 직접 스키마 정보를 구성할 수 있는 방법이 편리할 수 있습니다.</p>
<p>스파크 SQL 은 개발자들이 직접 스키마를 지정할 수 있는 방법을 제공하는데 리플렉션 방식을 설명할때 사용한 것과 동일한 데이터 프레임을 생성하는 예제를 통해 그 차이점을 보겠습니다.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">StructField sf1 = DataTypes.createStructField(<span class="string">"name"</span>, DataTypes.StringType, <span class="keyword">true</span>);</span><br><span class="line">StructField sf2 = DataTypes.createStructField(<span class="string">"age"</span>, DataTypes.IntegerType, <span class="keyword">true</span>);</span><br><span class="line">StructField sf3 = DataTypes.createStructField(<span class="string">"job"</span>, DataTypes.StringType, <span class="keyword">true</span>);</span><br><span class="line">StructType scheme = DataTypes.createStructField(Arrays.asList(sf1, sf2, sf3, sf4));</span><br><span class="line"></span><br><span class="line">Row r1 = RowFactory.create(<span class="string">"junhee"</span>, <span class="number">6</span> <span class="string">"student"</span>);</span><br><span class="line">Row r2 = RowFactory.create(<span class="string">"bill"</span>, <span class="number">12</span> <span class="string">"god"</span>);</span><br><span class="line">Row r3 = RowFactory.create(<span class="string">"jesi"</span>, <span class="number">14</span> <span class="string">"mom"</span>);</span><br><span class="line">Row r4 = RowFactory.create(<span class="string">"mina"</span>, <span class="number">41</span> <span class="string">"chef"</span>);</span><br><span class="line"></span><br><span class="line">List&lt;Row&gt; rows = Arrays.asList(r1, r2, r3, r4);</span><br><span class="line">Dataset&lt;Row&gt; df6 = spark.createDataFrame(rows, scheme);</span><br></pre></td></tr></table></figure>

<p>데이터 프레임의 스키마 정보는 <strong>칼럼을 나타내는 StructField 와 로우를 나타내는 StructType</strong> 으로 정의하는데, StructField 에는 칼럼의 이름과 타입, null 허용 여부를 지정하고, StructType 에는 칼럼으로 사용할 StructField 목록을 지정합니다.</p>
<p>StructField 와 StructType 정보를 구성햇다면, 데이터 프레임을 생성할 데이터를 준비합니다. 스키마 정보에 맞춰 Row 객체를 생성하는 과정입니다.</p>
<p>모든 준비가 끝나면, 스파크 세션이 제공하는 createDataFrame 메서드를 이용해 데이터프레임을 생성합니다.</p>
<h6 id="5-5-1-2-3-이미지-파일을-이용한-데이터-프레임-생성"><a href="#5-5-1-2-3-이미지-파일을-이용한-데이터-프레임-생성" class="headerlink" title="5.5.1.2.3 이미지 파일을 이용한 데이터 프레임 생성"></a>5.5.1.2.3 이미지 파일을 이용한 데이터 프레임 생성</h6><h4 id="5-5-2-주요-연산-및-사용법"><a href="#5-5-2-주요-연산-및-사용법" class="headerlink" title="5.5.2 주요 연산 및 사용법"></a>5.5.2 주요 연산 및 사용법</h4><p><strong>데이터 프레임은 org.apache.spark.sql.Row 타입의 객체로 구성된 데이터셋</strong>을 가리키는 용어입니다. 스칼라 API 에서는 데이터셋의 선언부를 보면 Dataset[T] 라고 되어 있는데, Dataset[Int], Dataset[String] 등은 그냥 데이터셋, 유일하게 Dataset[Row] 경우에만 데이터프레임이라는 용어를 사용합니다.</p>
<p><strong>이렇게 동일한 타입을 구분해서 사용하는 이유는 Dataset[Row] 인 경우 사용 가능한 트랜스포매이션 연산의 종류가 달라지기 때문</strong>입니다.</p>
<p>지금부터 데이터셋이 제공하는 주요 연산을 살펴보면서 데이터프레임에 특화된 것들은 어떤 것들이 있는지 확인합니다.</p>
<p>데이터셋이 제공하는 연산은 크게</p>
<ul>
<li>액션 연산</li>
<li>기본 연산</li>
<li>비타입 트랜스포매이션 연산<ul>
<li>데이터셋의 구성요소가 org.apache.spark.sql.Row 타입인 경우, 즉 <strong>데이터 프레임인 경우에만</strong> 사용 가능</li>
</ul>
</li>
<li>타입 트랜스포매이션 연산<ul>
<li><strong>데이터프레임이 아닌 데이터셋</strong>의 경우에만 사용 가능</li>
</ul>
</li>
</ul>
<h5 id="5-5-2-1-액션-연산"><a href="#5-5-2-1-액션-연산" class="headerlink" title="5.5.2.1 액션 연산"></a>5.5.2.1 액션 연산</h5><p>데이터셋은 RDD 와 마찬가지로 액션 연산과 트렌스포매이션 연산을 제공하며, 액션 연산이 호출될 경우 실제 여산인 수행됩니다. 이는 데이터 프레임이 제공하는 비타입 트렌스포메이션 연산의 경우도 마찬가지고, <strong>액션 연산을 호출해야만 트렌스포메이션 연산의 결과를 확인 할 수 있습니다</strong>.</p>
<ul>
<li>show()</li>
<li>head(), first()</li>
<li>take()</li>
<li>count()</li>
<li>collect(), collectAsList()</li>
<li>describe()</li>
</ul>
<h5 id="5-5-2-2-기본-연산"><a href="#5-5-2-2-기본-연산" class="headerlink" title="5.5.2.2 기본 연산"></a>5.5.2.2 기본 연산</h5><p>다음은 데이터 셋이 제공하는 기본 연산입니다.</p>
<ul>
<li>cache(), persist()</li>
<li>printScheme(), columns, dtypes, scheme</li>
<li>createOrReplaceTempView()</li>
<li>explain()</li>
</ul>
<h5 id="5-5-2-3-비타입-트랜스포메이션-연산"><a href="#5-5-2-3-비타입-트랜스포메이션-연산" class="headerlink" title="5.5.2.3 비타입 트랜스포메이션 연산"></a>5.5.2.3 비타입 트랜스포메이션 연산</h5><p><strong>데이터의 실제 타입을 사용하지 않는 변환 연산</strong>을 수행한다는 의미에서 붙여진 이름입니다.</p>
<p>동일한 Person 오브젝트로 구성된 RDD 와 데이터프레임을 준비하고 이들로부터 나이 값이 10 이상인 데이터만 조회할 경우 어떤 부분이 달라지는지 확인합니다.</p>
<p>다음처럼 RDD 의 경우, 정상 동작합니다.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala &gt; rdd.filter(p =&gt; p.age &gt; <span class="number">10</span>).collect.foreach(println)</span><br></pre></td></tr></table></figure>

<p>데이터프레임의 경우,</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala &gt; df.filter(p =&gt; p.age &gt; <span class="number">10</span>).show</span><br></pre></td></tr></table></figure>

<p>위와 같이 하면, “value age is not a member of org.apache.spark.sql.Row” 오류가 납니다. 이는 filter 함수에서 사용된 변수 p 가 Person 타입이 아닌, org.apache.spark.sql.Row 타입의 오브젝트 라는 뜻으로, 이를 통해 데이터프레임의 filter 함수 내부에서 참조하는 데이터들이 원래 데이터 타입인 Person 이 아니라는 것을 알 수 있습니다. </p>
<p>이처럼, 원래 데이터의 타입이 아닌 Row 타입을 사용해 트랜스 포매이션 연산을 수행하게 된다는 의미로 비타입 트랜스 포매이션 연산이라는 이름을 사용합니다.</p>
<p>다음은 기존 데이터 프레임에서 제공하던 연산에 해당하는 비타입 트렌스포매이션에 대한 것입니다.</p>
<ul>
<li>Row, Column, functions</li>
<li>!==, ===</li>
<li>alias(), as()</li>
<li>isin()</li>
<li>when()</li>
<li>max(), mean()</li>
<li>collect_list(), collect_set()</li>
<li>count(), countDistinct()</li>
<li>sum()</li>
<li>grouping(), grouping_id()</li>
<li>array_contains(), size(), sort_array()</li>
<li>explode(), posexplode()</li>
<li>current_date(), unix_timestamp(), to_date()</li>
<li>add_months(), date_add(), last_dat()</li>
<li>window()</li>
<li>round(), sqrt()</li>
<li>array()</li>
<li>desc(), asc()</li>
</ul>
<p>…..</p>
<h3 id="5-6-데이터셋"><a href="#5-6-데이터셋" class="headerlink" title="5.6 데이터셋"></a>5.6 데이터셋</h3><p>RDD 와 데이터프레임 이외에 다른 데이터 모델을 제시하는 가장 큰 이유는, 데이터프레임과 RDD 간의 데이터 타입을 다루는 방식의 차이때문입니다.</p>
<p>RDD 는 내부 데이터의 타입을 명확하게 정의해서 사용하도록 강제돼있는데 반해, <strong>데이터프레임의 경우 내부 데이터가 Row 의 집합이라는 것만 보장돼 있을 뿐 실제 데이터의 타입에 대한 정보는 외부에 노출되지 않습니다</strong>.</p>
<p>데이터프레임은 데이터셋과 다르지 않은 완전히 동일한 클래스입니다. 즉, 동일한 데이터를 서로 다른 방식으로 표현하기 위한 모델이지 서로 다른 것이 아닙니다. 실제로 데이터프레임과 데이터셋은 자유롭게 변환이 가능하며 둘 중 어떤 것을 사용하더라도 내부적인 구현은 동일한 방법을 따릅니다.</p>
<p>단, 데이터셋을 사용할 경우 스파크 내부에서 최적화가 가능한 스파크 내장 함수보다 사용자 정의 함수나 스칼라 또는 자바 언어의 다양한 외부 라이브러리를 사용하게 될 가능성이 높은데, 이 경우 자칫 성능엥 안 좋은 영향을 줄 위험이 있으므로 주의해서 사용해야 합니다.</p>
<h4 id="5-6-1-데이터셋-생성"><a href="#5-6-1-데이터셋-생성" class="headerlink" title="5.6.1 데이터셋 생성"></a>5.6.1 데이터셋 생성</h4><p>자바 객체 또는 기존 RDD, 데이터프레임으로부터 생성될 수 있습니다.</p>
<h5 id="5-6-1-1-파일로부터-생성"><a href="#5-6-1-1-파일로부터-생성" class="headerlink" title="5.6.1.1 파일로부터 생성"></a>5.6.1.1 파일로부터 생성</h5><p>외부 파일로부터 데이터셋을 생성하기 위해서는 스파크세션이 제공하는 read() 메서드를 이용합니다.</p>
<h5 id="5-6-1-2-자바-객체를-이용해-생성"><a href="#5-6-1-2-자바-객체를-이용해-생성" class="headerlink" title="5.6.1.2 자바 객체를 이용해 생성"></a>5.6.1.2 자바 객체를 이용해 생성</h5><p>앞에서 살펴본 리플렉션 방식의 데이터프레임 생성과 유사합니다.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Person row1 = <span class="keyword">new</span> Person(<span class="string">"hayoon"</span>, <span class="number">7</span>, <span class="string">"student"</span>);</span><br><span class="line">Person row2 = <span class="keyword">new</span> Person(<span class="string">"jko"</span>, <span class="number">12</span>, <span class="string">"teacher"</span>);</span><br><span class="line">Person row3 = <span class="keyword">new</span> Person(<span class="string">"junhee"</span>, <span class="number">14</span>, <span class="string">"worker"</span>);</span><br><span class="line">Person row4 = <span class="keyword">new</span> Person(<span class="string">"park"</span>, <span class="number">34</span>, <span class="string">"fireman"</span>);</span><br><span class="line"></span><br><span class="line">List&lt;Person&gt; data= Arrays.asList(row1,row2,row3,row4);</span><br><span class="line">Dataset&lt;Person&gt; df2 = spark.createDataset(data, Encoders.bean(Person<span class="class">.<span class="keyword">class</span>))</span>;</span><br></pre></td></tr></table></figure>

<p><strong>데이터셋을 생성할 때는 인코더 정보를 반드시 명시</strong>해야합니다. 인코더는 자바 객체와 스파크 내부 바이너리 포맷 간의 변환을 처리하기 위한 것으로 스파크 1.6 에서 데이터셋과 함께 처음 소개되었습니다. 인코더가 하는 역할은 기존 자바 직렬화 프레임워크나 Kyro 같이 자바 객체를 바이너리 포맷으로 변환하는 것입니다. </p>
<p>하지만, 기존 직렬화 프레임워크처럼 단순히 네트워크 전송 최적화를 위한 바이너리 포맷을 만드는 것에 그치는 것이 아니라, 데이터의 타입과 그 데이터를 대상으로 수행하고자 하는 연산, 데이터를 처리하고 있는 하드웨어 환경까지 고려한 최적화된 바이너리를 생성하고 다룬다는 점에서 차이가 있습니다.</p>
<h5 id="5-6-1-3-RDD-및-데이터프레임을-이용해-생성"><a href="#5-6-1-3-RDD-및-데이터프레임을-이용해-생성" class="headerlink" title="5.6.1.3 RDD 및 데이터프레임을 이용해 생성"></a>5.6.1.3 RDD 및 데이터프레임을 이용해 생성</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._ <span class="comment">// 스칼라의 경우, 기본 타입에 대한 인코더 정보를 암묵적 변환 방식을 이용해 제공</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"><span class="keyword">val</span> ds3 = spark.createDataset(rdd)</span><br><span class="line"><span class="keyword">val</span> ds4 = rdd.toDS()</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Person&gt; rdd= sc.parallelize(data);</span><br><span class="line">Dataset&lt;Row&gt; df3 = spark.createDataFrame(rdd, Person<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">Dataset&lt;Person&gt; ds4 = df3.as(Encoders.bean(Person<span class="class">.<span class="keyword">class</span>))</span>;</span><br></pre></td></tr></table></figure>

<h5 id="5-6-1-4-range-로-생성"><a href="#5-6-1-4-range-로-생성" class="headerlink" title="5.6.1.4 range() 로 생성"></a>5.6.1.4 range() 로 생성</h5><p>스파크세션이 제공하는 range() 메서를 이용해 연속된 숫자로 구성된 간단한 데이터셋을 생성할 수 있습니다.</p>
<h4 id="5-6-2-타입-트랜스포매이션-연산"><a href="#5-6-2-타입-트랜스포매이션-연산" class="headerlink" title="5.6.2 타입 트랜스포매이션 연산"></a>5.6.2 타입 트랜스포매이션 연산</h4><p>데이터셋이 제공하는 연산은</p>
<ul>
<li>기본연산</li>
<li>타입/비타입 트랜스포메이션 연산</li>
<li>액션 연산</li>
</ul>
<p>타입 트렌스포메이션 연산으로는</p>
<ul>
<li>select</li>
<li>as</li>
<li>distinct</li>
<li>dropDuplicates</li>
<li>filter</li>
<li>map, flatMap</li>
<li>groupByKey()</li>
<li>agg()</li>
<li>mapValues, reduceGroups</li>
<li>…</li>
</ul>
<h3 id="5-7-하이브-연동"><a href="#5-7-하이브-연동" class="headerlink" title="5.7 하이브 연동"></a>5.7 하이브 연동</h3><p>하이브는 SQL 기반의 데이터 처리 기능을 제공하는 오픈소스 라이브러리 프로그램 작성의 편리함과 HiveQL 이라고 하는 강력하고 다양한 기능의 쿼리 API 로 인해 널리 사용되고 있는 데이터웨어하우스 시스템입니다.</p>
<p>스파크SQL 은 하이브 시스템을 사용하지 않더라도 표준 SQL 이외에 하이브QL 을 사용할 수 있도록 지원하며, 필요한 경우 외부 하이브 시스템과 직접 연동해서 테이블을 공유하고 데이터를 주고 받는 방법도 제공합니다.</p>
<p>별도로 설치한 하이브 서버가 있고 그 서버와 연동해서 사용하도 싶다면 스파크홈의 conf 디렉토리 아래에 하이브와 하둡 설정 파일인 hive-site.xml , core-size.xml , hdfs-site.xml 파일을 복사하며 됩니다.</p>
<h3 id="5-8-분산-SQL-엔진"><a href="#5-8-분산-SQL-엔진" class="headerlink" title="5.8 분산 SQL 엔진"></a>5.8 분산 SQL 엔진</h3><p>스파크SQL 은 그 자체가 분산 SQL 서버로 동작해서, 다른 프로그램이 JDBC 또는 ODBC 방식으로 접속한 뒤 스파크에서 생성한 테이블을 사용할 수 있는 기능을 제공합니다. 따라서 이 기능을 사용하면 스파크 프로그램을 작성하지 않더라도 스파크 테이블을 대상으로 쿼리를 수행할 수 있습니다.</p>
<p>스파크 SQL 엔진을 실행한 뒤 하이브의 JDBC 클라이언트 프로그램인 beeline 을 이용해 테이블을 조회할 수 있습니다.</p>
<h3 id="5-9-Spark-SQL-CLI"><a href="#5-9-Spark-SQL-CLI" class="headerlink" title="5.9 Spark SQL CLI"></a>5.9 Spark SQL CLI</h3><p>스파크가 제공하는 명령행 툴입니다.</p>
<h3 id="5-10-Query-Plan-과-디버깅"><a href="#5-10-Query-Plan-과-디버깅" class="headerlink" title="5.10 Query Plan 과 디버깅"></a>5.10 Query Plan 과 디버깅</h3><p>RDD 라는 훌륭한 API 가 있음에도 데이터프레임/데잉터셋 이라는 새로운 API 가 등장하게 된것은 데이터를 처리하는 과정의 최적화를 통한 성능 개선 가능성 때문입니다.</p>
<h4 id="5-10-1-SparkSession-SessionState-SparkContext"><a href="#5-10-1-SparkSession-SessionState-SparkContext" class="headerlink" title="5.10.1 SparkSession / SessionState / SparkContext"></a>5.10.1 SparkSession / SessionState / SparkContext</h4><p>스파크세션 객체 안에는 스파크 컨텍스트 객체가 포함되어 있기 때문에, RDD 를 만들 때나 데이터프레임을 만들 때나 상관없이 스파크 세션 객체를 먼저 생성하면 됩니다.</p>
<p>스파크세션 객체를 만들려면 스파크컨텍스트를 비롯한 SessionState 객체가 먼저 생성되어 있어야하며, 실제로 어딘가에 이부분을 처리하는 코드가 있습니다. </p>
<p><strong>즉, 스파크세션은 스파크컨텍스트에 세션상태 정보(SessionState) 를 추가로 담은 것이라고 할 수 있습니다.</strong></p>
<p>그렇다면, 스파크세션에 세션 상태 정보라는 것을 추가한 이유는 ? 스파크 컨텍스트는 스파크가 동작하기 위한 각종 백엔드 서비스에 대한 참조를 가지고 있는 객체이기 때문입니다.</p>
<p>스파크에서 동작하는 모든 어플리케이션은 백엔드 서버와 통신하기 위해 스파크컨텍스트 객체를 사용해야하며 이 같은 이유로 스파크센션의 경우에도 스파크컨텍스트를 먼저 생성한 후 이에 대한 참조를 내부적으로 유지하고 있는 것입니다.</p>
<h4 id="5-10-2-QueryExecution"><a href="#5-10-2-QueryExecution" class="headerlink" title="5.10.2 QueryExecution"></a>5.10.2 QueryExecution</h4><p>데이터프레임을 사용할 경우 스파크가 내부적으로 최적화 과정을 거쳐 실제 실행 코드를 생성합니다. 스파크에서는 이런 변환 과정에서 일어나는 일을 사용자가 확인할 수 있게 특화된 API 를 제공하는데 대표적인 것이 queryExecution 메서드 입니다.</p>
<h4 id="5-10-3-LogicalPlan-과-SparkPlan"><a href="#5-10-3-LogicalPlan-과-SparkPlan" class="headerlink" title="5.10.3 LogicalPlan 과 SparkPlan"></a>5.10.3 LogicalPlan 과 SparkPlan</h4><p>스파크는 최적화 쿼리를 수행할 때 논리적 실행 계획을 나타내는 LogicalPlan 과 구체적 실행 계획을 나타내는 SparkPlan 이라는 두개의 클래스를 사용합니다. </p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2019-07-15T15:00:00.000Z" title="2019-07-15T15:00:00.000Z">2019-07-16</time><span class="level-item"><a class="link-muted" href="/categories/spark/">Spark</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/07/16/rdd/">[스파크2 프로그래밍] 2장_RDD</a></h1><div class="content"><h3 id="2-1-RDD"><a href="#2-1-RDD" class="headerlink" title="2.1 RDD"></a>2.1 RDD</h3><p>이번 장의 목표 : 데이터 모델로서의 추상적인 RDD 가 아닌, 프로그램 작성을 위한 API 관점에서 RDD 를 이해</p>
<h4 id="2-1-1-들어가기에-앞서"><a href="#2-1-1-들어가기에-앞서" class="headerlink" title="2.1.1 들어가기에 앞서"></a>2.1.1 들어가기에 앞서</h4><p>RDD 다루기 전에 알아야 할 것</p>
<ol>
<li><p>스파크 클러스터</p>
<p>클러스터 환경에서 동작하는 프로그램 작성할 때는 데이터가 여러 서버에 나눠져 병렬로 처리됩니다.</p>
</li>
<li><p>분산데이터로서의 RDD</p>
<p>RDD 는 회복력을 가진 분산 데이터 집합입니다.</p>
</li>
<li><p>불변성</p>
<p>한번 만들어진 RDD 는 어떤 경우에도 변경되지 않습니다.</p>
</li>
<li><p>파티션</p>
<p>RDD 데이터는 클러스터를 구성하는 여러 서버에 나누어서 저장됩니다. 스파크는 분할된 데이터를 파티션이라는 단위로 관리합니다.</p>
</li>
<li><p>HDFS</p>
</li>
<li><p>Job 과 Executor</p>
<p>스파크 프로그램을 실행하는 것을 스파크 잡을 실행한다고 합니다. 하나의 잡은 클러스터에서 병렬로 처리되고, 각 서버마다 익스큐터라는 프로세스가 생성됩니다. 각자 할당된 파티션을 처리합니다.</p>
</li>
<li><p>드라이버 프로그램</p>
<p>드라이버란, 스파크 컨텍스트를 생성하고 그 인스턴스를 포함하는 있는 프로그램입니다.</p>
</li>
<li><p>트랜스포메이션과 액선</p>
<p>트랜스포메이션은 RDD 의 형태를 변형하는 연산입니다. 액선은 어떤 동작을 수행해 그 결과로서 RDD 가 아닌 다른 타입의 결과를 변환하는 연산입니다.</p>
</li>
<li><p>지연 동작과 최적화</p>
<p>트랜스포메이션 연산은 RDD 를 사용하는 다른 액션 연산이 호출될때까지는 실제 트랜스포메이션을 수행하지 않습니다. 따라서, 실행 계획의 최적화가 가능합니다. <strong>사용자가 입력한 변환 연산들을 즉시 수행하지 않고 모아뒀다가 한 번에 실행함으로써 불필요한 네티워크 통신 비용을 줄일 수 있습니다.</strong></p>
</li>
<li><p>함수의 전달</p>
</li>
</ol>
<h4 id="2-1-2-스파크-컨텍스트-생성"><a href="#2-1-2-스파크-컨텍스트-생성" class="headerlink" title="2.1.2 스파크 컨텍스트 생성"></a>2.1.2 스파크 컨텍스트 생성</h4><p>스파크컨텍스트는 <strong>스파크 애플리케이션과 클러스터의 연결을 관리하는 객체</strong>로서 스파크 애플리케이션은 반드시 스파크 컨텍스트를 생성해야합니다. 클러스터 마스터 정보와 애플리케이션 이름은 반드시 지정해야하는 필수정보입니다.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SparkConf conf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"RDDCreateSample"</span>);</span><br><span class="line">JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br></pre></td></tr></table></figure>

<h4 id="2-1-3-RDD-생성"><a href="#2-1-3-RDD-생성" class="headerlink" title="2.1.3 RDD 생성"></a>2.1.3 RDD 생성</h4><p>RDD 생성 방법 두가지가 있습니다.</p>
<ol>
<li><p>드라이버 프로그램의 컬렉션 객체 이용</p>
<p>컬렉션 객체는 자바나 파이썬의 경우에는 리스트 타입을 사용합니다.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; rdd = sc.parallelize(Arrays.asList(<span class="string">"a"</span>,<span class="string">"b"</span>,<span class="string">"c"</span>,<span class="string">"d"</span>,<span class="string">"e"</span>));</span><br></pre></td></tr></table></figure>
</li>
<li><p>파일과 같은 외부 데이터 이용</p>
<p>스파크는 내부적으로 하둡의 입력 및 출력 기능을 사용하므로 하둡이 다룰 수 있는 모든 입출력 유형을 다룰 수 있습니다.</p>
<p>파일의 각 줄은 한 개의 RDD 구성요소가 됩니다. 파일을 읽어들이는 과정은 하둡의 TextInputFormat 을 이용합니다. </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;String&gt; rdd = sc.textFile(<span class="string">"&lt;spark_home_dir&gt;/README.md"</span>);</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="2-1-4-RDD-기본-액션"><a href="#2-1-4-RDD-기본-액션" class="headerlink" title="2.1.4 RDD 기본 액션"></a>2.1.4 RDD 기본 액션</h4><h5 id="2-1-4-1-collect"><a href="#2-1-4-1-collect" class="headerlink" title="2.1.4.1 collect"></a>2.1.4.1 collect</h5><p>RDD 의 모든 원소를 모아서 배열로 돌려줍니다. 반환 타입이 RDD 가 아닌 배열이므로 이 연산은 액션에 속하는 연산입니다. RDD 에 있는 모든 요소들이 collect 연산을 호출한 서버의 메모리에 수집됩니다. 따라서 충분한 메모리 공간이 확보되어야합니다.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>));</span><br><span class="line">List&lt;Integer&gt; result = rdd.collect();</span><br><span class="line"><span class="keyword">for</span> (Integer i : result) System.out.println(i);</span><br></pre></td></tr></table></figure>

<h5 id="2-1-4-2-count"><a href="#2-1-4-2-count" class="headerlink" title="2.1.4.2 count"></a>2.1.4.2 count</h5><p>RDD 구성하는 전체 요소 개수 반환합니다.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>));</span><br><span class="line"><span class="keyword">long</span> result = rdd.count();</span><br><span class="line">System.out.println(result);</span><br></pre></td></tr></table></figure>

<h4 id="2-1-5-RDD-트랜스포메이션"><a href="#2-1-5-RDD-트랜스포메이션" class="headerlink" title="2.1.5 RDD 트랜스포메이션"></a>2.1.5 RDD 트랜스포메이션</h4><p><strong>기존 RDD 를 이용해 새로운 RDD 를 생성</strong>하는 연산입니다.</p>
<ul>
<li>Map 연산<ul>
<li>요소간의 mapping 을 정의한 함수를 RDD 에 속하는 모든 요소에 적용해 새로운 RDD 를 생성</li>
</ul>
</li>
<li>그룹화 연산<ul>
<li>특정 조건에 따라 요소를 그룹화 하거나 특정 함수를 적용</li>
</ul>
</li>
<li>집합 연산<ul>
<li>RDD 에 포함된 요소를 하나의 집합으로 간주할 때 서로 다른 RDD 간에 합집합, 교집합 등을 계산</li>
</ul>
</li>
<li>파티션 연산<ul>
<li>RDD 의 파티션 개수를 조정</li>
</ul>
</li>
<li>필터, 정렬 연산<ul>
<li>특정 조건을 만족하는 요소만 선택하거나 각 요소를 정해진 기준에 따라 정렬</li>
</ul>
</li>
</ul>
<h4 id="2-1-6-RDD-액션"><a href="#2-1-6-RDD-액션" class="headerlink" title="2.1.6 RDD 액션"></a>2.1.6 RDD 액션</h4><p>RDD 메서드 중에서 <strong>결과값이 정수나 리스트, 맵 등 RDD가 아닌 다른 타입</strong>인 것들입니다. </p>
<p>트렌스포메이션에 속하는 메서드는 느긋한 평가 방식을 사용합니다. 즉, 호출한다고 즉시 실행되는 것이 아니라 액션으로 분류되는 메서드가 호출되어야하만 비로소 실행됩니다. 액션 메서드를 호출하는 시점이 돼서야 비로소 그동안 쌓여있던 ~개의 트렌스포메이션 연산이 순차적으로 시작됩니다.</p>
<p><strong>주의할점은, 액션 메서드를 여러번 호출하면 트렌스포메이션 메서드도 여러번 실행됩니다.</strong> 예를 들어, rdd1 이라는 RDD 에 map() 연산을 적용해 rdd2 라는 RDD 를 만들었다고 할때, rdd2 의 액션 메서드를 두번 호출하면 map() 연산도 두번 실행됩니다. 따라서, 반복 수행 성능을 개선하기 위해 캐쉬를 적절히 사용하고, 코드 작성시 반복 수행 가능성을 염두해야합니다.</p>
<h4 id="2-1-7-RDD-데이터-불러오기와-저장하기"><a href="#2-1-7-RDD-데이터-불러오기와-저장하기" class="headerlink" title="2.1.7 RDD 데이터 불러오기와 저장하기"></a>2.1.7 RDD 데이터 불러오기와 저장하기</h4><p>스파크는 하둡 API 를 기반으로 다양한 데이터 포맷과 파일 시스템을 지원합니다.</p>
<ul>
<li>파일 포맷<ul>
<li>텍스트 파일 / JSON / 하둡의 시퀀스 파일 / csv …</li>
</ul>
</li>
<li>파일 시스템<ul>
<li>로컬 파일 시스템 / HDFS / AWS 의 S3 / 오픈스택의 Swift …</li>
</ul>
</li>
</ul>
<h5 id="2-1-7-1-텍스트-파일"><a href="#2-1-7-1-텍스트-파일" class="headerlink" title="2.1.7.1 텍스트 파일"></a>2.1.7.1 텍스트 파일</h5><p>스파크는 다양한종류의 파일 시스템을 다룰수 있기 때문에 파일의 경로를 지정하는 방법도 파일 시스템의 종류에 따라 다릅니다.</p>
<ul>
<li>로컬파일 시스템<ul>
<li>file:///path</li>
</ul>
</li>
<li>HDFS<ul>
<li>hdfs://master:prot/path/..</li>
</ul>
</li>
<li>S3<ul>
<li>S3n://bucket/path</li>
</ul>
</li>
</ul>
<p><strong>주의할점은, 스파크가 클러스터를 이루는 다수의 서버 상에서 동작하기 때문에,  위에서 지정한 경로는 클러스터를 구성하는모든 서버에서 동일하게 접근 가능해야합니다.</strong> 따라서, 로컬 파일 시스템의경로를 데이터 위치로 지정하면 클러스터를 구성하는 모든 서버에서 “file:///data/sample.txt” 라는 경로를 통해 지정한 파일로 접근할 수 있어야합니다.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">JavaRdd&lt;Integer&gt; rdd = sc.parallelize(fillToN(<span class="number">1000</span>),  <span class="number">3</span>); <span class="comment">// 0~1000 까지의 숫자로 구성, 3개 파티션</span></span><br><span class="line">Class codec =. org.apache.hadoop.io.compress.GzipCodec<span class="class">.<span class="keyword">class</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// save</span></span><br><span class="line">rdd.saveAsTextFile(<span class="string">"&lt;path_to_save&gt;/sub1"</span>);</span><br><span class="line">rdd.saveAsTextFile(<span class="string">"&lt;path_to_save&gt;/sub2"</span>,  codec);</span><br><span class="line"></span><br><span class="line"><span class="comment">// load</span></span><br><span class="line">JavaRDD&lt;String&gt; rdd2 = sc.textFile(<span class="string">"&lt;path_to_save&gt;/sub1"</span>);</span><br></pre></td></tr></table></figure>

<h5 id="2-1-7-2-Object-File"><a href="#2-1-7-2-Object-File" class="headerlink" title="2.1.7.2 Object File"></a>2.1.7.2 Object File</h5><p>텍스트 파일을 사용하는 것과. 크게 다르지 않습니다. 다만, RDD 에 포함된 데이터를 오프젝트 파일로 다루기 위해서는 각 요소(오브젝트) 가 자바의 Serializable 인터페이스를 구현하고 있어야합니다. 그리고, 저장된 RDD 의 타입이 RDD[Int]  였다면, 이 파일으 읽어서 생성한 RDD 도 동일한 RDD[Int] 타입입니다.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">JavaRdd&lt;Integer&gt; rdd = sc.parallelize(fillToN(<span class="number">1000</span>),  <span class="number">3</span>); <span class="comment">// 0~1000 까지의 숫자로 구성, 3 개 파티션</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// save</span></span><br><span class="line">rdd.saveAsObjectFile(<span class="string">"&lt;path_to_save&gt;/sub_path"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// load</span></span><br><span class="line">JavaRDD&lt;Integer&gt; rdd2 = sc.objectFile(<span class="string">"&lt;path_to_save&gt;/sub_path"</span>);</span><br><span class="line">System.out.println(rdd2.take(<span class="number">10</span>));</span><br></pre></td></tr></table></figure>

<h5 id="2-1-7-3-시퀀스-파일"><a href="#2-1-7-3-시퀀스-파일" class="headerlink" title="2.1.7.3 시퀀스 파일"></a>2.1.7.3 시퀀스 파일</h5><p>키와 값으로 구성된 데이터를 저장하는 이진 파일 포맷입니다. 하둡에서 자주 사용되는 대표적인 파일 포맷입니다. 시퀀스 파일로다루고자 하는 RDD의 데이터는 하둡의 Wriable 인터페이스를 구현하고 있어야합니다.</p>
<h4 id="2-1-8-클러스터-환경에서의-공유-변수"><a href="#2-1-8-클러스터-환경에서의-공유-변수" class="headerlink" title="2.1.8 클러스터 환경에서의 공유 변수"></a>2.1.8 클러스터 환경에서의 공유 변수</h4><p>하둡이나 스파크와 같이 클러스터 환경에서 동작하는 애플리케이션은 하나의 잡을 수행하기 위해 클러스터에 속한 다수의 서버에서 여러 개의 프로세스를 실행하므로 모든 프로세스가 공유할 수 있는 자원을 관리하기 쉽지 않습니다. <strong>이러한 프레임워크는 다수의 프로세스가 공유할 수 있는 읽기 자원과 쓰기 자원을 설정할 수 있도록 지원합니다.</strong></p>
<ul>
<li>하둡<ul>
<li>분산캐시 / 카운터</li>
</ul>
</li>
<li>스파크<ul>
<li>브로드캐스트 변수 / 어큐뮬레이텨</li>
</ul>
</li>
</ul>
<ol>
<li><p>브로드캐스트 변수</p>
<p>스파크 잡이 실행되는 동안 클러스터 내의 모든 서버에서 공유할 수 있는 읽기 전용 자원을 설정할 수 있는 변수입니다.</p>
<ul>
<li>먼저 공유하고자 하는 데이터를 포함하는 오브젝트를 생성</li>
</ul>
</li>
</ol>
<ul>
<li><p>이 오브젝트를 스파크컨텍스트의 broadcast() 메서드의 인자로 지정해 해당 메서드를 실행</p>
<ul>
<li><p>이렇게 생성된 브로드캐스트 변수를 사용할때는 생성한 브로드캐스트 변수의 value() 메서드를 통해 접근</p>
<p>클러스터 간에 공유할 변수가 있다고 해서 무조건 브로드캐스트 변수를 사용해야하는 것은 아닙니다. <strong>액션 연산을 수행할때 동일한 스테이지 내에서 실행되는 태스크 간에는 필요한 변수를 자동으로 브로드캐스트 변수를 이용해서 전달</strong>하기 때문에 명시적으로 브로드 캐스트 변수를 지정하지않아도 됩니다. </p>
</li>
</ul>
</li>
</ul>
<ol start="2">
<li><p>어큐뮬레이터</p>
<p>쓰기 동작을 위한것입니다. 클러스터 내의 모든 서버가 공유하는 쓰기 공간을 제공함으로써 <strong>각 서버에서 발생하는 특정 이벤트의 수를 세거나 관찰하고 싶은 정보를 모아두는 용도로 활용할 수 있습니다.</strong> </p>
<p>어큐뮬레이터를 생성하려면 org.apache.spark.util.AccumulatorV2 클래스를 상속받은 클래스를 정의하고, 이 클래스의 인스턴스를 생성합니다. 그리고 생성한 어큐뮬레이터 인스턴스를 스파크컨텍스트가 제공하는 register() method 를 이용해 등록합니다. </p>
<p>어큐뮬레이터를 사용할 때는 두 가지를 기억해야합니다.</p>
<p>첫 째, 어큐뮬레이터를 증가시키는 동작은 클러스터의 모든 데이터 처리 프로세스에서 가능하지만 데이터를 읽는 동작은 드라이버 프로그램 내에서만 가능합니다. 즉, RDD 의 트랜스포메이션이나 액션 연산 내부에는 어큐뮬레이터의 값을 증가시킬뿐 그 값을 참조해서 사용하는 것은 불가능합니다. </p>
<p>둘 째, 일부러 의도한 특별한 목적이없는 한 어큐뮬레이터는 액션 연산을 수행하는 메서드에서만 사용해야합니다. 왜냐하면 트렌스포매이션 연산은 액션 연산과 달리 하나의 잡 내에서 필요에 따라 수차례 반복 실행될 수 있기 때문입니다. 따라서 map(), flatmap() 과 같은 트랜스포메이션 연산 내용에 어큐뮬레이터의 값을 증가시키는 코드가 포함될 경우 정확하지 않은 데이터가 집계 될 수 있습니다.</p>
</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2019-06-30T15:00:00.000Z" title="2019-06-30T15:00:00.000Z">2019-07-01</time><span class="level-item"><a class="link-muted" href="/categories/spark/">Spark</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/07/01/spark-cluster/">[스파크2 프로그래밍] 3장_클러스터 환경</a></h1><div class="content"><h3 id="3-1-클러스터-환경"><a href="#3-1-클러스터-환경" class="headerlink" title="3.1 클러스터 환경"></a>3.1 클러스터 환경</h3><p>이번 장의 목표 : 분산 처리를 위한 시스템 아키텍처와, 그와 관련된 다양한 설정 및 매개변수를 이해하는 것.</p>
<p>스파크에서는 클러스터 자원을 관리해주는 역할을 하는 컴포넌트 클래스를 클러스터 매니저라고 합니다. 2.3.0 버젼에서는 네 종류의 클러스터 매니저가 사용되고 있습니다.</p>
<h4 id="3-1-1-클러스터-모드와-컴포넌트"><a href="#3-1-1-클러스터-모드와-컴포넌트" class="headerlink" title="3.1.1 클러스터 모드와 컴포넌트"></a>3.1.1 클러스터 모드와 컴포넌트</h4><p>스파크 클러스터는 <strong>드라이버 / 클러스터 매니저 / 익스큐터 / 워커 노드</strong>의 조합.</p>
<p>클러스터란, 여러 대의 서버가 네트워크를 통해 연결되어 마치 하나의 서버인 것처럼 동작하는 방식을 의미합니다. 전체 서버의 자원과 동작을 세밀하고 효율적으로 제어할 수 있는 별도 모듈을 클러스터 매니저라고 부릅니다.</p>
<p>스파크에서는  추상화된 클러스터 모델을 제공함으로써 사용하는 클러스터의 종류에 상관없이 일관된 방법으로 프로그램을 작성하고 클러스터를 관리할 수 있게 지원하고 있습니다.</p>
<p>“스파크 애플리케이션을 실행했다” 라고 하는 말은, 드라이버 프로그램에 있는 메인 함수를 실행해 스파크컨텍스트를 생성하고, 이를 이용해 각 워커 노드에 익스큐터 프로세스를 구동시켜 작업을 수행했다라는 뜻입니다. 즉, 익스큐터 하나가 사용할 자원(CPU 나 메모리) 을 정한뒤, 작업 실행 요청이 발생할 때마다 필요한 수만큼의 익스큐터를 할당하는 방식으로 자원을 할당합니다.</p>
<p>사용가능한 스파크컨텍스트가 준비돼 있다는 것은 클러스터 메니저와의 연동을 포함해서, 스파크 애플리케이션이 동작하는 데 필요한 다수의 서비스가 준비돼 있다는 의미이며, 이렇게 생성된 스파크컨텍스트를 이용해 RDD 나 브로드캐스트 또는 어큐뮬레이터 변수를 생성하고 사용할수 있음을 의미하는 것입니다.</p>
<p>쇼핑몰 방문자의 방문 로그를 분석하는 작업을 한다고 가정하고 작업을 수행하는 절차를 보겠습니다.</p>
<ol>
<li>드라이버 프로그램이 포함된 애플리케이션 코드를 작성</li>
<li>코드를 빌드하고 jar 나 zip 파일 등으로 패키징</li>
<li>생선한 패키지 파일을 스파크에서 제공하는 spark-submit 셸을 이용해 클러스터에 배포하고 실행</li>
<li>스파크 애플리케이션의 드라이버 프로그램이 실행되면 스파크컨텍스트가 생성되면서 클러스터 매니저와 연동되어 각 클러스터 서버에 작업을 처리하기 위한 프로세스를 생성. 이때 작업에 필요한 서버를 워커노드라고 하며, 각 워커노드에 생성된 프로세스를 익스큐터.</li>
<li>익스큐터가 생성되면 드라이버 프로그램은 작성된 프로그램에 의해 트렌스포메이션과 액션을 수행. 트렌스포메이션 연산이 호출할 때는 실제 작업을 수행하지 않고 액션 여산이 호출될 때만 실제 작업을 수행하느데, 이 작업 단위를 Job. 즉, 잡은 액션 연산의 수만큼 생성.</li>
<li>생성된 잡은 실제로 수행될 때 스테이지라는 단계로 나누어 실행. 스테이지를 나누는 기준이 되는 것은 데이터 셔플 필요 여부. 즉, 각 서버에 있는 데이터를 네트워크를 통해 다른 서버로 재배치해야 하는지 여부. 셔플이 발생하면 네트워크를 통해 대량의 데이터를 정렬하고 전송하는 등의 부하가 발생해 전체 작업 성능에 좋지 않은 영향을 끼치기 때문. 따라서, 데이터를 이동하지 않는 상태에서 처리할 수 있는 연산을 최대한 같은 스테이지로 묶어 처리하면 셔플 발생을 최소화.</li>
<li>각 스테이지는 여러 개의 태스크로 나눠진 후 분산처리를 위해 여러 익스큐터에 할당되며, 이 태스크가 실제 익스큐터에 전달되는 작업의 단위. 이 때, 익스큐터는 두 가지 역할 수행. 하나는 할당받은 테스크를 처리. 다른 하나는 이미 처리된 데이터를 나중에 빠르게 재사용할 수 있게 메모리에 저장.</li>
</ol>
<h4 id="3-1-2-클러스터-모드를-위한-시스템-구성"><a href="#3-1-2-클러스터-모드를-위한-시스템-구성" class="headerlink" title="3.1.2 클러스터 모드를 위한 시스템 구성"></a>3.1.2 클러스터 모드를 위한 시스템 구성</h4><p>일반적으로, 별도의 서버에 애플리케이션을 배포한 뒤 해당 서버에서 드라이버 프로그램을 구동하고 실제 데이터 처리는 스파크 클러스터에서 수행되게 하는 방법을 사용합니다. 이렇게, <strong>클러스터에 작업을 요청하는 서버를 배치 서버 또는 클라이언트 서버라고 부릅니다.</strong> </p>
<p>다음은 클러스터 구성에 필요한 서버의 종류입니다.</p>
<ol>
<li><p>로컬 개발 서버</p>
</li>
<li><p>애플리케이션 실행 서버</p>
<p>spark-submit, spark-shell 등의 스크립트를 이용해 스파크 어플리케이션을 맨 처음 실행하는 서버.</p>
</li>
<li><p>클러스터 서버</p>
<p>클러스터 구성에 참여하는 서버. 클러스터 운영을 위한 마스터 서버의 역할을 수행하거나, 실제 데이터를 처리하고 필요에 따라 저장하는 워커 노드의 역할을 수행하는 서버입니다. </p>
</li>
</ol>
<h4 id="3-1-3-드라이버-프로그램과-디플로이-모드"><a href="#3-1-3-드라이버-프로그램과-디플로이-모드" class="headerlink" title="3.1.3 드라이버 프로그램과 디플로이 모드"></a>3.1.3 드라이버 프로그램과 디플로이 모드</h4><p>모든 스파크 어플리케이션에는 스파크컨텍스트를 생성하는 코드가 포함돼 있는데, 이 부분이 포함된 프로그램을 가리켜 드라이버 프로그램이라고 합니다. 클러스터에서 실행할 때는 클러스터 매니저에게 애플리케이션 실행을 요청합니다. (“제출한다” 라고 합니다.)</p>
<p>작업 요청을 받은 클러스터 매니저는 필요한 자원을 할당하고 작업을 수행하는데, 클러스터 매니저마다 다른 형태로 애플리케이션을 실행시킬 수 있습니다. 이처럼 서로 다른 실행 모드를 ‘디플로이 모드’ 라고 합니다. ‘클라이언트 디플로이 모드’ 와 ‘클러스터 디플로이 모드’ 가 있습니다.</p>
<p><strong>클라이언트 디플로이 모드란, 애플리케이션을 실행한 프로세스 내부에서 드라이버 프로그램을 구동하는 것</strong>으로, 드라이버 프로그램은 작업을 요청한 클라이언트 서버 프로세스에 포함되어 실행됩니다. 따라서 스파크 어플리케이션을 실행했던 콘솔을 닫아 버리거나 기타 다른 방법으로 프로세스를 중지시키면 스파크컨텍스트도 함께 종료되면서 수행 중이던 모든 스파크 잡이 중지 됩니다.</p>
<p><strong>클러스터 디폴로이 모드란, 애플리케이션을 실행한 프로세스는 클러스터 매니저에게 작업 실행만 요청하고 즉시 종료되며, 실제 드라이버 프로그램의 실행은 클러스터 내부에서 실행되는 것</strong>을 의미합니다. 클러스터 매니저에게 잡이 전달되고 최초 어플리케이션을 실행했던 콘솔들 닫아 버리거나 기타 다른 방법으로 프로세스를 중지시켜도 전체 스파크 어플리케이션의 동작에는 영향을 끼치지 않습니다.</p>
<h3 id="3-2-클러스터-매니저"><a href="#3-2-클러스터-매니저" class="headerlink" title="3.2 클러스터 매니저"></a>3.2 클러스터 매니저</h3><p>스파크의 클러스터 모드를 구성하는 컴포넌트. 중 하나로 여러 대의 서버로 구성된 클러스터 환경에서 다수의 어플리케이션이 함께 구동될 수 있게 어플리케이션 간의 CPU 나 메모리, 디스크와 같은 컴퓨팅 자원을 관리해주는 역할을 합니다. 하둡의 Yarn 이나 아파치 Mesos 등이 있습니다.</p>
<h4 id="3-2-1-스탠드얼론-클러스터-매니저"><a href="#3-2-1-스탠드얼론-클러스터-매니저" class="headerlink" title="3.2.1 스탠드얼론 클러스터 매니저"></a>3.2.1 스탠드얼론 클러스터 매니저</h4><p>스파크나 하둡과 같이 클러스터 환경에서 동작하는 대부분의 프레임워크는 실행 모드라는 개념을 가지고 있습니다. 즉, 개발 및 테스트를 위해서는 1대의 단독 서버 혹은 개인 PC 에서 애플리케이션을 실행하고, 실 서비스에서는 여러 서버로 구성된 클러스터 환경에서 동일한 어플리케이션을 실행할 수 있는 것입니다.</p>
<h5 id="3-2-1-1-개요"><a href="#3-2-1-1-개요" class="headerlink" title="3.2.1.1 개요"></a>3.2.1.1 개요</h5><p>스탠드얼론 클러스터 매니저는 마스터/슬레이브 개념으 도입해 <strong>하나의 마스터 인스턴스와 다수의 슬레이브 인스턴스로 클러스터를 구성</strong>합니다. 이를 스파크의 클러스터 컴포넌트 모넬 관점에서 보면, 마스터 인스턴스가 클러스터 매니저 컴포넌트에 해당하고 슬레이브 인스턴스는 워커 노드에 해당합니다.</p>
<p>마스터는 클러스터 매니저의 역할을 담당해서 <strong>클라이언트의 요청을 받아 필요한 서버 자원을 할당하고 슬레이브의 작업 실행을 관리하는 기능을 수행</strong>합니다. 슬레이브는 Worker 의 역학을 담당하면서 <strong>Executor 와 Task 를 실행해 데이터에 대한 실제 처리와 저장</strong>을 수행합니다.</p>
<p>클러스터를 시작하는 방법은, 마스터 인스턴스를 구동한 뒤에. 마스터의 접속 주소를 슬레이브 인스턴스의 실행 인자로 전달하면서 슬레이브 프로세스를 구동시킵니다. 스파크 어플리케이션을 동작시키는 경우에는 마스터 서버의 주소를 maseter 속성 값으로 지정해서 실행합니다.</p>
<h5 id="3-2-1-2-설치"><a href="#3-2-1-2-설치" class="headerlink" title="3.2.1.2 설치"></a>3.2.1.2 설치</h5><p>스탠드얼론 클러스터 매니저를 사용하려면 클러스터를 구성하는 모든 서버에 스파크가 설치되어 있어야합니다.</p>
<p>설치가 끝나면 먼저 마스터 인스턴스를 실행한 후 슬레이브 인스턴스를 실행합니다. 실행 스크립트를 이용하려면, 마스터와 슬레이브 프로세스가 실행된 서버 간에 패스워드 없이 SSH 접속이 가능하도록 설정해야합니다.</p>
<h5 id="3-2-1-3-클러스터-매니저-실행"><a href="#3-2-1-3-클러스터-매니저-실행" class="headerlink" title="3.2.1.3 클러스터 매니저 실행"></a>3.2.1.3 클러스터 매니저 실행</h5><p>마스터 서버에 접속한 뒤 스파크 홈 아래에 있는 sbin 디렉토리로 이동합니다. 이 디렉토리에는 스탠드얼론 클러스터 매니저의 실행과 종료를 위한 다수의 실행 스크립트가 있습니다.</p>
<ul>
<li>마스터 인스턴스와 슬레이브 인스터스를 개별적으로 실행할 때 사용할 수 있는 스크립트</li>
<li>마스터 서버를 비롯한 다수의 슬레이브 인스턴스를 한번에 실행하거나 종료할 때 사용하는 스크립트</li>
</ul>
<h5 id="3-2-1-4-애플리케이션-실행-서버-준비"><a href="#3-2-1-4-애플리케이션-실행-서버-준비" class="headerlink" title="3.2.1.4. 애플리케이션 실행 서버 준비"></a>3.2.1.4. 애플리케이션 실행 서버 준비</h5><p>이제 클러스터 매니저를 이용해 원하는 작업을 실행해볼 차례입니다. 작업을 실행하는 방법은 세 가지가 있는데, 그 전에 애플리케이션을 배포해서 실행할 서버를 결정해야합니다. 클러스터를 구성하는 서버 중 하나를 사용할 수도 있고 별도의 서버를 사용할 수 있는데 일반적으로 별도의 서버를 사용합니다. </p>
<h5 id="3-2-1-5-애플리케이션-실행"><a href="#3-2-1-5-애플리케이션-실행" class="headerlink" title="3.2.1.5 애플리케이션 실행"></a>3.2.1.5 애플리케이션 실행</h5><p>스파크 셸 / pyspark / spark-submit 세 가지 방법이 있습니다.</p>
<h6 id="3-2-1-5-1-스파크-셸"><a href="#3-2-1-5-1-스파크-셸" class="headerlink" title="3.2.1.5.1 스파크 셸"></a>3.2.1.5.1 스파크 셸</h6><p>컴파일이나 배포 과정을 거치지 않고 셸에서 직접 코드를 입력하고 그 결과를 즉시 확인할 수 있는 인터랙티브한 개발환경을 제공함으로써 코드의 작성과 디버깅을 손쉽게 할 수 있습니다.</p>
<p>스파크 셸도 일종의 스파크 어플리케이션이기 때문에 마스터 정보를 전달해야합니다.  클러스터 마스터 URL 이 ‘spark://svr01:7077’ 이면, 애플리케이션 실행 서버에서 아래와 같이 실행합니다.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/spark-shell --master spark://svr01:7077</span><br></pre></td></tr></table></figure>

<h6 id="3-2-1-5-2-pyspark"><a href="#3-2-1-5-2-pyspark" class="headerlink" title="3.2.1.5.2 pyspark"></a>3.2.1.5.2 pyspark</h6><p>스파크 셸과 같은 기능을 수행하며 파이썬 언어를 사용하는 경우에 사용할 수 있습니다.</p>
<h6 id="3-2-1-5-3-spark-submit"><a href="#3-2-1-5-3-spark-submit" class="headerlink" title="3.2.1.5.3 spark-submit"></a>3.2.1.5.3 spark-submit</h6><p>스파크 어플리케이션을 실행하는데 필요한 각종 설정 값을 명확하고 일관된 방식으로 정의할 수 있기 때문에, 자바 스칼라 파이썬 등의 언어로 작성된 어플리케이션을 동일한 방법으로 실행가능합니다.</p>
<h5 id="3-2-1-6-디플로이-모드"><a href="#3-2-1-6-디플로이-모드" class="headerlink" title="3.2.1.6 디플로이 모드"></a>3.2.1.6 디플로이 모드</h5><p>디플로이 모드를 지정하는 방법은 클러스터 매니저의 종류에 따라 다를수 있는데 스탠드얼론 매니저를 사용할 경우 –deploy-mode 옵션을 사용해 cluster 와 client 모드 중 하나를 지정할 수 있습니다.</p>
<h5 id="3-2-1-7-주요-설정"><a href="#3-2-1-7-주요-설정" class="headerlink" title="3.2.1.7 주요 설정"></a>3.2.1.7 주요 설정</h5><h5 id="3-2-1-8-HA"><a href="#3-2-1-8-HA" class="headerlink" title="3.2.1.8 HA"></a>3.2.1.8 HA</h5><p>스탠드얼론 클러스터 모드는 하나의 마스터 + 다수의 워쿼 로 구성됩니다. 마스터는 각 워커에 작업을 지시하고, 작업 수행 상태를 모니터링하다가 워커 중 하나에 문제가 생기면, 그 워카가 수행하던 작업을 다른 워커에 전달해서 전체 작업이 문제 없이 처리되게 합니다. 하지만 워커가 아닌 마스터에 문제가 생기면 작업은 복구하지 못하고 실패합니다.</p>
<p>스탠드얼론 클러스터 모드는 주키퍼를 사용해 다수의 마스터 서버를 동일한 클러스터의 마스터로 지정하고, 문제가 발생할 경우 다른 마스터로 전환할 수 있게 함으로써 단일 마스터 서버 운영으로 인한 장애 발생 가능성을 낮출 수 있습니다.</p>
<p>#####3.2.1.9 단일 노드 복구</p>
<p>로컬 디렉토리의 특정 위치에 클러스터의 상태 정보를 저장해뒀다가 장애가 발생하면 저장된 정보를 이용해 다시 예전 상태를 복구하는 방식입니다. </p>
<h4 id="3-2-2-아파치-매소스"><a href="#3-2-2-아파치-매소스" class="headerlink" title="3.2.2 아파치 매소스"></a>3.2.2 아파치 매소스</h4><h4 id="3-2-3-얀"><a href="#3-2-3-얀" class="headerlink" title="3.2.3 얀"></a>3.2.3 얀</h4><p>하둡에서 제공하는 클러스터 자원 관리 서비스입니다. 얀의 등장 배경을 살펴봅시다.</p>
<p>하둡은 잡 트랙커와 테스크 트래커 프로세스를 이용해서 하둡의 대표적인 애플리케이션인 맵리듀스 어플리케이션을 실행하는 방법을 사용하고 있었습니다. 두 프로세스는 마스터와 슬레이브 관계로, 하나의 잡 트레커가 다수의 태스크 트레커를 관리하는 형태로 동작했습니다. 클러스터에서 수행되는 모든 잡은 잡 틀래커 프로세스를 통해 처리됐습니다.</p>
<p>하지만 잡 트래커는 전체 클러스터에서 하나만 실행됐기 때문에 <strong>하나의 클러스터에서 여러 개의 에플리케이션이 실행될 경우 하나의 잡 트래커가 전체 어플리케이션의 자원 할당, 실행 제어, 모니터링, 히스토리 관리에 이르는 모든 처리를 수행해야하는 문제</strong>가 있었습니다.</p>
<p>결국 이렇게 하나의 마스터 프로세스에 의존하는 배치 프로세싱 모델은 마스터 프로세스의 한계가 전체 클러스터의 한계로 이어지는 결과를 가져왓습니다.</p>
<p>얀은 이러한 기존 맵리듀스 프레임워크의 문제점을 개선하기 위해 제안된 것으로, <strong>잡 트래러가 가지고 있던 자원 관리와 작업처리 모니터링 및 히스토리 관리 기능을 각각 별개의 서비스로 분리</strong>한 것입니다. </p>
<h5 id="3-2-3-1-개요"><a href="#3-2-3-1-개요" class="headerlink" title="3.2.3.1 개요"></a>3.2.3.1 개요</h5><p>크게 봤을 때, 얀은 <strong>클라이언트 프로그램 / 리소스 메니저 / 노드 매니저 / 애플리케이션 마스터 / 컨테이너</strong> 등의 컴포넌트로 나눠볼 수 있습니다.</p>
<p>얀 어플리케이션 실행 과정은 다음과 같습니다.</p>
<ol>
<li>얀은 전체 크러스터의 자원을 관리하는 리소스 매니저와 각 노드의 자원을 관리하는 노드 매니저라는 두 종류의 데몬 프로세스로 구성돼있습니다. 노드 매니저는 클러스터를 구성하는각 노드에서 동작하는 데몬으로 해당 노드의 자원을 관리하고 노드가 보유하고 있는 자원 현황을 주기적으로 리소스 메니저에게 보고해서 리소트 메니저가 전체 클러스터의 자원현황을 알 수 있게 합니다. 리소스 매니저는 노드 매니저로부터 각 노드의 리소스 현황을 보고 받고 이를 관리하며 클러스터 내의 모든 어플리케이션의 실행에 필요한 자원을 할당하고 관리합니다.</li>
<li>클라이언트 프로그램은 리소스 매니저에게 어플리케이션 (==어플리케이션 마스터) 를 실행해달라고 요청하는 프로그램입니다. 일반적으로 클라이언트 프로그램을 실행하는 서버는 클러스터에 속하지 않는 별도의 외부서버입니다.</li>
<li>클라이언트 프로그램으로부터 어플리케이션 실행 요청을 받은 리소스 매니저는 노드 매니저 중에서 어플리케이션 마스터를 실행할 수 있는, 가용 자원이 있는 노드 매니저를 찾아서 어플리케이션 마스터의 실행을 요청합니다. 실행된 어플리케이션 마스터는 리소스 매니저에게 등록하는 절차를 거치며, 리소스 매니저는 이렇게 등록된 어플리케이션 마스터를 통해 전체 리소스를 관리합니다.</li>
<li>리소스 매니저로부터 어플리케이션 마스터 실행 요청을 받은 노드 매니저는 어플리케이션 마스터 생성에 필요한 자원을 컨테이너라는 단위로 할당하고 컨테이너에 할당된 자원 (CPU 나 메모리) 를 이용해 어플리케이션 마스터를 실행한 뒤 자원 할당 결과를 리소스 메니저에게 보고합니다.</li>
<li>어플리케이션 마스터가 실행되면 해당 어플이케이션에서 필요한 작업을 수행합니다. </li>
</ol>
<p>스파크의 클러스터 컴포넌트 모델과 얀의 컴포넌트 모델은 어떤 관계가 있을까?</p>
<ol>
<li>클라이언트 프로그램이 실행되면서 얀 리소스 매니저에게 애플리케이션을 실행해 줄것을 요청하면 리로스 매니저가 적절한 노드 매니저 하나를 선택해 애플리케이션 마스터를 실행. 이때 어플리케이션 마스터는 스파크 애플리케이션을 위해 작성된 것으로 클러스터 디플로이 방식이면 드라이버 프로그램이 같은 프로세스 내에서 수행.</li>
<li>애플리케이션 마스터는 리소스 매니저에게 스파크 애플리케이션 실행에 필요한 자원을 요청해 이를 수행시킬 노드 목록을 전달받고, 해당 노드 매니저에게 필요한 자원 할당 및 프로세스 실행 요청.</li>
<li>애플리케이션 마스터로부터 애플리케이션 프로세스 실행 요청을 받은 노드 매니저들은 필요한 자원은 담고 있는 컨테이너를 할당하고 요청받은 애플리케이션 프로세스 실행. 실행되는 애플리케이션 프로세스가 곧 스파크의 익스큐터.</li>
<li>익스큐터가 실행되면 스파크 드라이버 프로그램이 생성된 익스큐터 프로세스를 이용해 스파크의 테스트 수행.</li>
</ol>
<h5 id="3-2-3-2-설치"><a href="#3-2-3-2-설치" class="headerlink" title="3.2.3.2 설치"></a>3.2.3.2 설치</h5><h5 id="3-2-3-3-스파크-잡-실행"><a href="#3-2-3-3-스파크-잡-실행" class="headerlink" title="3.2.3.3 스파크 잡 실행"></a>3.2.3.3 스파크 잡 실행</h5><p>다른 클러스터 매니저와 같습니다. 다만, 클러스터 마스터 URL 을 직접 지정하는 방법이 다릅니다.</p>
<h6 id="3-2-3-3-1-스파크-쉘"><a href="#3-2-3-3-1-스파크-쉘" class="headerlink" title="3.2.3.3.1 스파크 쉘"></a>3.2.3.3.1 스파크 쉘</h6><p>마스터에 대한 접속 정보를 하둡의 설정 파일을 통해 읽어드리므로, –master 매개변수에 그냥 ‘yarn’ 이라고만 입력하고 대신 하둡의 설정파일 (core-site.xml, yarn-site.xml 등) 이 있는 디렉토리 경로를 ‘HADOOP_CONF_DIR’ 또는 ‘YARN_CONF_DIR’ 이라는 환경변수로 등록해야합니다.</p>
<h5 id="3-2-3-3-2-pyspark"><a href="#3-2-3-3-2-pyspark" class="headerlink" title="3.2.3.3.2 pyspark"></a>3.2.3.3.2 pyspark</h5><h5 id="3-2-3-3-3-spark-submit"><a href="#3-2-3-3-3-spark-submit" class="headerlink" title="3.2.3.3.3 spark-submit"></a>3.2.3.3.3 spark-submit</h5><h5 id="3-2-3-4-디플로이-모드"><a href="#3-2-3-4-디플로이-모드" class="headerlink" title="3.2.3.4 디플로이 모드"></a>3.2.3.4 디플로이 모드</h5><p>다른 클러스터 매니저와 크게 다르지 않아서, –deploy-mode 매개변수의 값으로 client 나 cluster 를 지정합니다.</p>
<p>클라이언트 디플로이 모드이면, 얀 클라이언트 프로그램에서 실행되는 드라이버가 애플리케이션 동작을 제어하고 얀의 어플리케이션 마스터는 단순히 노드 매니저에게 필요한 자원을 요청하는 역할을 합니다.</p>
<h5 id="3-2-3-5-얀-컨테이너-로그-설정"><a href="#3-2-3-5-얀-컨테이너-로그-설정" class="headerlink" title="3.2.3.5 얀 컨테이너 로그 설정"></a>3.2.3.5 얀 컨테이너 로그 설정</h5><p>스파크 어플리케이션이 실행되면 드라이버 프로그램이 제공하는 (드라이버 프로그램이 생성한 스파크 컨텍스트가 제공하는) 모니터링 페이지를 통해 해당 어플리케이션의 모니터링 및 디버깅에 필요한 각종 정보를 확인할 수 있습니다.</p>
<h4 id="3-2-4-히스토리-서버와-매트릭스"><a href="#3-2-4-히스토리-서버와-매트릭스" class="headerlink" title="3.2.4 히스토리 서버와 매트릭스"></a>3.2.4 히스토리 서버와 매트릭스</h4><p>스파크컨텍스트가 제공하는 모니터링 페이지는 애플리케이션이 실행 중일 때만 접속가능합니다.</p>
<p>스파크에서는 이렇게 애플리케이션이 종료된 후에도 과거 이벤트 로그를 활용해 어플리케이션에 대한 정보를 볼 수 있게 하는 히스토리 서버를 제공합니다.</p>
<p>“–master” 옵션은 스파크가 사용할 <strong>클러스터의 마스터 정보를 지정하는 옵션</strong>입니다. 사용하는 클러스터 마스터 ( 혹은 메니저) 정보를 지정하는 옵션입니다.</p>
<p>만약 클러스터가 아닌 단일서버에서 동작시킬 경우에 “local” 이라고 입력합니다. 이 경우, 스파크 잡은 하나의 서버에서 하나의 스레드만 이용해서 동작합니다. 따라사 여러개의 스레들르 이용하려면 “local[2]” 처럼 지정합니다. 스레드 두 개를 사용한다는 의미입니다. “local[*]” 는 사용 가능한 모든 스레드를 사용합니다.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2019-06-30T15:00:00.000Z" title="2019-06-30T15:00:00.000Z">2019-07-01</time><span class="level-item"><a class="link-muted" href="/categories/spark/">Spark</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/07/01/spark-config/">[스파크2 프로그래밍] 4장_스파크 설정</a></h1><div class="content"><ul>
<li>애플리캐이션 단위로 설정<ul>
<li>Spark Properties 사용</li>
</ul>
</li>
<li>각 서버 단위로 설정<ul>
<li>서버의 환경 변수를 이용해 등록</li>
</ul>
</li>
</ul>
<h3 id="4-1-스파크-프로퍼티"><a href="#4-1-스파크-프로퍼티" class="headerlink" title="4.1 스파크 프로퍼티"></a>4.1 스파크 프로퍼티</h3><p>스파크 프로퍼티는 개별 애플리캐이션 실행관 관련된 설정값을 정의하는 곳입니다. 스파크 컨텍스트를 생성할 때 사용했던 SparkConf 인스턴스나 자바 시스템 프로퍼티를 이용해 등록 가능합니다.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkCont</span>().setAppName(<span class="string">"myApp"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br></pre></td></tr></table></figure>

<p>SparkConf 클래스는 스파크 애플리캐이션 실행관 관련된 다양한 설정 정보를 키와 값 형태로 등록할수 있는 함수(set / get) 을 제공합니다. 애플리캐이션 이름과 같이 반드시 지정해야 하는 주요 속성에 대해서는 setMaster, setAppName 과 같은 별도의 메서드를 제공합니다.</p>
<p>SparkConf 를 이용하는 것의 문제점은 애플리캐이션의 비즈니스 로직과는 직접 관련이 없는 익스큐터의 메모리 설정이나 코어 수 할당관 관련된 부분이 항상 프로그램 코드에 포함돼 있어야 한다는 것입니다. 그래서, 프로그램이 실행되는 시점에 동적으로 필요한 설정값을 설정할수 있는 두 가지 방법이 있습니다.</p>
<ol>
<li><p>spark-shell / spar-submit</p>
<p>스크립트 실행시 사전에 지정된 형식에 따라 명령행 옵션을 이용해 원하는 설정값을 지정합니다.</p>
</li>
<li><p>설정 정보가 담긴 파일 사용</p>
<p>스파크 홈의 conf 디렉터리 아래에 spark-defaults.conf 파일을 만들고 이 파일에 설정 정보를 등록해 두면 스파크 쉘이나 sprak-submit 스크립트가 해당 파일의 내용을 읽습니다. </p>
</li>
</ol>
<h3 id="4-2-환경변수"><a href="#4-2-환경변수" class="headerlink" title="4.2 환경변수"></a>4.2 환경변수</h3><p>각 서버 단위로 적용돼야 하는 환경 정보는 각 서버의 환경 변수를 이용해 등록할 수 있습니다. 예를 들어, 자바의 설치 경로와 같은 정보가 서버별로 다르게 설정돼 있다면 환경변수를 이용해 해당 서버의정보를 변경할 수 있습니다. 환경 변수 ex )</p>
<ul>
<li>JAVA_HOME : 자바 설치 경로</li>
<li>SPARK_LOCAL_IP : 사용할 IP</li>
<li>SPARK_PUBLIC_DNS : 애플리케이션 호스트명</li>
<li>SPARK_CONF_DIR : spar-defaults.conf / spark-env.sh / log4j.properties 파일 등 설정 파일이 놓인 디렉토리 위치</li>
</ul>
<p>일부 환경 변수는 사용하는 클러스터 매니저의 종류에 따라 설정 방법이 다릅니다.</p>
<h3 id="4-3-로깅-설정"><a href="#4-3-로깅-설정" class="headerlink" title="4.3 로깅 설정"></a>4.3 로깅 설정</h3><p>스파크는 로깅 프레임워크로 log4j 를 사용합니다. 로깅 레벨을 변경하고 싶으면 스파크 홈의 conf 디렉토리 아래에 log4j.properties 파일을 생성하고 원하는 레벨로 설정하면 됩니다.</p>
<h3 id="4-4-스케쥴링"><a href="#4-4-스케쥴링" class="headerlink" title="4.4 스케쥴링"></a>4.4 스케쥴링</h3><p>하나의 애플리캐이션에 무조건 많은 CPU 와 대량의 메모리를 할당한다고 해서 원하는 속도가 나온다는 보장이 없습니다. 오히려, GC 발생과 과도한 IO, 네트워크, 프로세스 경항 등으로 인해 처리 속도가 더 나빠질 수 있습니다.</p>
<p>이번 절에서는 스파크 애플리캐이션을 수행할 때 클러스터 자원을 각 자원에 적절히 분배해서 사용하는 방법을 정리합니다.</p>
<p>하나의 클러스터에서 여러 작업이 실행되는 경우는 두 가지 입니다.</p>
<ol>
<li>서로 다른 애플리캐이션이 동일한 클러스터에서 동시에 실행</li>
<li>하나의 애플리캐이션에서 여러 스레드를 이용해 다수의 잡을 동시에 실행</li>
</ol>
<h4 id="4-4-1-애플리캐이션-간의-자원-스케쥴링"><a href="#4-4-1-애플리캐이션-간의-자원-스케쥴링" class="headerlink" title="4.4.1 애플리캐이션 간의 자원 스케쥴링"></a>4.4.1 애플리캐이션 간의 자원 스케쥴링</h4><ol>
<li><p>애플리캐이션 단위로 고정된 자원을 할당해주는 고정 자원 할당 방식</p>
<p>각 애플리케이션별마다 할당된 자원을 미리 결정한뒤 애플리케이션이 실행되는 시점부터 종료되는 시점까지 해당 자원을 계속 점유하는 사용방식입니다. </p>
<p>스파크셸 이나 spark-submit 을 이용해 애플리케이션을 실행할 때 사용할 자원 정보를 지정할 수 있습니다.</p>
<p>만약, 실행되는 애플리케이션이 짧은 시간에 집중적으로 처리를 수행하는 애플리케이션이 아닌, 스파크 셸이나 웹 기반의 애플리케이션 처럼 장시간 동작하면서 사람 혹은 외부 프로스세가 제공하는 이벤트가 있을 때만 작업을 처리하는 형태로 동작하는 애플리케이션이라면 외부로부터의 명령을 대기하는 시간 동안 자원의 낭비가 발생합니다.</p>
<p>따라서, 작업을 수행하지 않고 외부의 작업 요청을 대기하는 동안에는 해당 자원을 회수해서 자원이 부족한 다른 애플리케이션에 추가로 할당하는 것이 합리적입니다.</p>
</li>
<li><p>애플리캐이션의 실행 상황에 따라 수시로 할당량을 조정해 주는 동적 자원 할당 방식</p>
<p>스탠드얼론 모드를 제외하고는 모두 별도의 셔플 서비스를 구동시킵니다. 왜냐하면, 동적 자원 할당 모드에서는 애플리케이션이 실행되고 있는 도중에 익스큐터가 스케쥴러에 의해 삭제될 수 있기 떄문입니다.</p>
<p>아직 리듀서가 읽어가지 않은 데이터를 갖고 있던 익스큐터가 스케쥴러에 의해 삭제되면 셔플 데이터가 유실되기 때문에 익스큐터가 삭제되더라도 해당 데이터를 유지하고 처리될 수 있게 별도의 셔플 프로세스를 설정하는 것입니다.</p>
</li>
</ol>
<h4 id="4-4-2-단일-애플리캐이션-내부에서의-자원-스케쥴링"><a href="#4-4-2-단일-애플리캐이션-내부에서의-자원-스케쥴링" class="headerlink" title="4.4.2 단일 애플리캐이션 내부에서의 자원 스케쥴링"></a>4.4.2 단일 애플리캐이션 내부에서의 자원 스케쥴링</h4><p>스파크컨텍스트는 기본적으로 멀티스레드 방식을 지원합니다. 하나의 스파크컨텍스트에서 다수의 액션 연산을 동시에 실행하더라도 문제가 없습니다. 하지만, 별도의 설정이 없다면 잡의 실행은 기본적으로 FIFO 방식입니다. 문제점은 먼저 시작되는 작업이 우선권을 얻어 자원을 모두 점유한 채 실핼됭 경우, 후속 작업은 이전 작업이 모두 완료되지 전까지 대기하고 있어야한다는 점입니다.</p>
<p>스파크는 그래서 FIFO 스케쥴러와 fail-scheduler 를 선택할 수 있는 옵션을 제공합니다. </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conf.set(<span class="string">"spark.scheduler.mode"</span>, <span class="string">"FAIR"</span>)</span><br></pre></td></tr></table></figure>

<p>페어 스케쥴링 방식을 사용하면, 모듭 잡은 동일한 자원을 번갈아가며 할당받습니다. 크기가 작은 잡과 수행 시간이 오래 걸리는 잡이 섞여 있을 때 크기가 작은 잡이 크기가 큰 잡을 기다리지 않고 빠른 처리가 가능합니다. 하지만 경우에 따라서는 중요한 작업과 덜 중요한 적업을 구분해서 자원 할당의 우선순위를 조정해야 하는 경우도 있습니다.</p>
<p>그래서, Pool 이라는 개념을 도입해 풀마다 스케쥴링 방식과 우선순위, 사용 가능한 자원 할당 수준을 다르게 설정한 뒤 각 잡을 특정 풀에 할당해 풀 단위로 작업을 관리할 수 있습니다.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2019-06-22T15:00:00.000Z" title="2019-06-22T15:00:00.000Z">2019-06-23</time><span class="level-item"><a class="link-muted" href="/categories/spark/">Spark</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/06/23/spark-intro/">[스파크2 프로그래밍] 1장_스파크 소개</a></h1><div class="content"><h3 id="1-1-스파크"><a href="#1-1-스파크" class="headerlink" title="1.1 스파크"></a>1.1 스파크</h3><h4 id="1-1-2-빅데이터의-정의"><a href="#1-1-2-빅데이터의-정의" class="headerlink" title="1.1.2 빅데이터의 정의"></a>1.1.2 빅데이터의 정의</h4><p>“다앙햔 형태를 지닌 대량의 데이터가 빠른 속도로 쌓이고 있다면 이를 빅데이터라고 부를 수 있다”</p>
<p>빅데이터의 중요한 특성 세 가지는 크기, 속도, 댜양성입니다.</p>
<ul>
<li>크기 : 대량의 데이터를 처리</li>
<li>속도 : 데이터의 증가가 지속적이고 빠르기 때문에 이에 부합하는 빠른 데이터 처리 기술이 필요</li>
<li>다양성 : 빅데이터의 다양성</li>
</ul>
<h4 id="1-1-3-빅데이터-솔루션"><a href="#1-1-3-빅데이터-솔루션" class="headerlink" title="1.1.3 빅데이터 솔루션"></a>1.1.3 빅데이터 솔루션</h4><p>먼저 빅데이터를 처리하는 플랫폼이니만큼 데이터를 가져오는 데이터 수집 모듈이 필요합니다. 다음으로는, 이렇게 수집된 데이터를 저장하고 조회하는 저장 및 조회 모듈이 필요합니다. 다음으로, 데이터를 분석하고 그 결과를 가공할 수 있는 모듈이 필요합니다. 그리고, 이제 이 모든 과정을 제어할 수 있는 워크플로우 엔진이 필요할 수도 있습니다. </p>
<ul>
<li>데이터 수집 : 플럼 / 카프카 / 스쿱</li>
<li>데이터 저장 및 처리 : 하둡 / HBase / 카산드라 / 레디스 / 피그 / 하이브 / 스파크</li>
<li>데이터 분석 및 기타 소프트웨어 : R / 클라우데라 / 엘라스틱서치</li>
</ul>
<h4 id="1-1-4-스파크"><a href="#1-1-4-스파크" class="headerlink" title="1.1.4 스파크"></a>1.1.4 스파크</h4><h5 id="하둡이란"><a href="#하둡이란" class="headerlink" title="하둡이란"></a>하둡이란</h5><p>빅데이터라는 용어가 이렇게 대중적으로 알려지게 된 데는 하둡의 탄생과 성공이 크게 기여했습니다. 하둡은 구글이 대용량 처리와 관련해서 공개한 두 개의 논문을 Doug Cutting 이 실제 제품으로 구현하면서 시작된 아파치 프로젝트를 가리키는 이름입니다. </p>
<p>하둡은 여러 대의 서버를 이용해서 하나의 클러스터를 구성하며, 이렇게 클러스터로 묶인 서버의 자원을 하나의 서버처럼 사용할 수 있는 클러스터 컴퓨팅 환경을 제공합니다. 기본 동작 방법은 <strong>분석할 데이터를 하둡 파일 시스템인 HDFS 에 저장해 두고, HDFS 상에서 Map Reduce 프로그램을 이용해 데이터 처리를 수행하는 방식</strong>입니다.</p>
<p>데이터를 저장할 때는 전체 데이터를 ‘블록’ 이라고 하는 일정한 크기로 나눠서 여러 데이터 노드에 분산해서 저장합니다. 이 때, 각 블록들이 어느 데이터 노드에 저장돼 있는지에 대한 메타정보를 네임 노드에 기록합니다. 그리고 맵 리듀스 작업을 실행할때는 네임노드로부터 메타정보를 읽어서 처리할 데이터의 위치를 확인하고 분산 처리를 수행합니다.</p>
<h5 id="맵리듀스란"><a href="#맵리듀스란" class="headerlink" title="맵리듀스란"></a>맵리듀스란</h5><p>맵리듀스 프레임워크는 하둡의 대표적인 데이터 처리 프레임워크입니다. 데이터를 여러 개 의 맵 프로세스와 리듀서 프로세스로 나눠서 처리하는 방식입니다. <strong>맵 프로세스는 여러 데이터 노드에서 분산 저장된 데이터를 각 서버에서 병렬로 나누어서 처리하며, 리듀서는 그러한 맵 프로세스들의 결과를 조합해 최종 결과를 만들어냅니다</strong>.</p>
<p>맵리듀스 잡의 제어는 네임노드에서 구동되는 잡 스케쥴러와 태스크 스케쥴러라는 프로세스가 처리했습니다. 하지만, 기본적으로 하나의 클러스터에서 한개의 맵리듀서 잡만 구동할 수 있었기 때문에 CPU와 메모리를 효율적으로 사용하지 못했습니다. 그래서, 하둡 2.0 부터 데이터 처리 작업에 대한 스케쥴링과 서버 자원 관리를 YARN 이라는 자원 관리 시스템에서 전담하면서 이러한 문제점이 개선됐습니다. </p>
<p>하둡만으로는 모든 데이터 처리를 수행하기에는 부족한 부분이 있었습니다.</p>
<ul>
<li>하둡의 맵리듀스 잡은 대부분의 연산 작업을 파일 시스템 기반으로 처리해서, 스파크 같은 메모리 기반 데이터 처리 방식에 비해 상대적으로 높은 성능을 기대하기 어려웠습니다.</li>
<li>맵리듀스 잡을 이용해서 데이터를 처리하려면 대부분 자바 언어를 사용해서 맵리듀스 프로그램을 작성해야했습니다.</li>
<li>외부 라이브러리의 도움 없이 단위 테스트를 작성하거나 실제 데이터를 대상으로 간단한 시뮬레이션을 하기 어려웠습니다.</li>
<li>SQL on Hadoop 인 하이브의 경우, 개발자들에게 친숙한 SQL 을 사용해서 맵리듀스 잡을 생성할 수 있지만, 이를 위해서는 미리 사용할 테이블과 데이터를 설계해야합니다.</li>
</ul>
<p>스파크는 하둡과 달리 <strong>메모리를 이용한 데이터 저장 방식을 제공</strong>함으로써 머신러닝 등 반복적인 데이터 처리가 필요한 분야에서 높은 성능을 보여줬습니다. 또한, <strong>작업을 실행하기 전에 최적의 처리 흐름을 찾는 과정</strong>으로 성능 향과 더불어 여러 개의 맵리듀스 잡을 직접 순차적으로 실행해야하는 수고를 덜 수 있게 됐습니다. 특히, 맵리듀스에 비해 훨씬 자연스럽고 강력한 다수의 데이터 처리 함수를 제공함으로써 프로그램의 복잡도를 낮춰줍니다. 또한, 스파크 2.0 부터 자바, 스칼라, 파이썬 뿐만 아니라 R 스크립트를 이용해서도 스파크 어플리케이션을 작성할수 있게 됐습니다.</p>
<h4 id="1-1-5-RDD-Dataset-DataFrame-소개와-연산"><a href="#1-1-5-RDD-Dataset-DataFrame-소개와-연산" class="headerlink" title="1.1.5 RDD, Dataset, DataFrame 소개와 연산"></a>1.1.5 RDD, Dataset, DataFrame 소개와 연산</h4><p>스파크 프로그램 내에서 데이터를 표현하고 처리하기 위한 프로그래밍 모델을 제공하는에 용도에 따라, RDD / Dataset / DataFrame 이라는 세가지 모델을 제공합니다.</p>
<p>RDD 는 스파크에서 정의한 분산 데이터 모델로서 병렬 처리가 가능한 요소로 구성되며 데이터를 처리하는 과정에서 프로그램 오류가 아닌 메모리 공간 부족 등의 이유로 일시적인 문제가 발생하더라도 <strong>스스로 에러를 복구 할 수 있는 능력을 가진 데이터 모델</strong>입니다.</p>
<p>스파크는 RDD 가 생성되어 변경되는 모든 광정을 일일이 기억 (Lineage) 하는 대신에 RDD 를 한번 생성되면 변경되지 않는 읽기 전용 모델로 만든 후 RDD 생성과 관련된 내용만 기억하고 있다가 장애가 발생하면 이전에 RDD 를 만들 때 수행했던 작업을 똑같이 실행해 (똑같은 데이터를 가진 새로운 RDD를 만들어) 데이터를 복구하는 방식을 사용하는 것입니다.</p>
<p>RDD 는 크게 세가지 방법으로 생성할 수 있습니다.</p>
<ul>
<li>List 나 Set 같은 기존 프로그램의 메모리에 생성된 데이터를 이용하는 것입니다.</li>
<li>로컬 파일시스템이나 하둡의 HDFS 같은 외부 저장소에 저장된 데이터를 읽어서 생성합니다.</li>
<li>기존에 생성되어 있는 RDD 로부터 또 다른 RDD 를 생성하는 방법입니다.</li>
</ul>
<p>RDD 를 생성하고 나면, RDD 가 제공하는 다양한 연산을 이용해 데이터를 처리하면 되는데, RDD 에서 제공하는 연산은 크게 Transformation 과 Action 이라는 두 종류로 나눌 수 있습니다.</p>
<ul>
<li>Transformation<ul>
<li>어떤 RDD 에 변형을 가해 새로운 RDD 를 생성하는 연산입니다. 변환 연산은 연산이 호출되는 시점에 바로 실행되는 것이 아니라, 변환을 어떻게 수행할 것인지에 대한 정보만 누적해서 가지고 있다가 Action 에 해당하는 연산이 호출될 때 한꺼번에 실행됩니다. 따라서 최종 실행이 필요한 시점에 누적된 변환 연산을 분석하고 그 중에서 최적의 방법을 찾아 변환 연산을 실행할 수 있습니다.</li>
</ul>
</li>
<li>Action<ul>
<li>연산의 결과로 RDD 가 아닌 다른 값을 반환하거나 아예 반환하지 않는 연산입니다.</li>
</ul>
</li>
</ul>
<h4 id="1-1-6-DAG"><a href="#1-1-6-DAG" class="headerlink" title="1.1.6 DAG"></a>1.1.6 DAG</h4><p>여러개의 꼭지점 또는 노드와 그 사이를 이어주는 방향성을 지닌 선으로 구성되고, 그래프를 구성하는 어느 꼭지점이나 노드에서 출발하더라도 다시 원래의 꼭지점으로 돌아오지 않도록 구성된 그래프 모델입니다.</p>
<p>스파크는 트렌스포메이션과 액션의 조합으로 데이터 흐름을 손쉽게 표현할 수 있습니다. 스파크에서 DAG 처리를 담당하는 부분을 DAG 스케쥴러라고합니다. 스케쥴러의 동작방식을 이해하기 위해서는 스파크의 작업 실행이 어떻게 수행되는지 이해해야합니다.</p>
<h5 id="스파크-작업-실행-순서-Driver-gt-DAG-Scheduler-gt-Cluster-Manager"><a href="#스파크-작업-실행-순서-Driver-gt-DAG-Scheduler-gt-Cluster-Manager" class="headerlink" title="스파크 작업 실행 순서 : Driver -&gt; DAG Scheduler -&gt; Cluster Manager"></a>스파크 작업 실행 순서 : Driver -&gt; DAG Scheduler -&gt; Cluster Manager</h5><p>스파크는 전체 작업을 Stage 라는 단위로 나누고, 각 스테이지를 다시 여러 개의 테스크로 나누어 실행합니다. 이 때, 최초의 메인함수를 실행해 RDD 등을 생성하고 각종 연산을 호출하는 프로그램을 Driver 프로그램이라고 합니다. Driver 프로그램은 메인 함수를 가진 일발적인 프로그램으로 작성하면 됩니다.</p>
<p>드라이버의 메인 함수에서는 스파크 애플리케이션과 스파크 클러스터의 연동을 담당하는 SparkContext 또는 SparkSession 이라는 객체를 만들고 이를 이용해 잡을 실행하고 종료하는 역할을 수행합니다. 드라이버가 스파크컨텍스트를 통해 RDD 의 연산정보를 DAG 스케쥴러에게 전달하면 스케쥴러는 이 정보를 가지고 실행 계획을 수립한 후에 이를 클러스터 메니저에게 전달합니다. 이 때, 스케쥴러가 생성하는 정보는 주로 데이터에 대한 지역성을 높이는 전략과 관련된것입니다. 전체 데이터 처리 흐름을 분석해서 네트워크를 통한 데이터 이동이 최소화 되도록 스테이지를 구성하는 것을 주로 수행합니다.</p>
<h4 id="1-1-7-람다-아키텍쳐"><a href="#1-1-7-람다-아키텍쳐" class="headerlink" title="1.1.7 람다 아키텍쳐"></a>1.1.7 람다 아키텍쳐</h4><p>빅데이터 처리를 위한 시스템을 구성하는 방법 중 하나입니다. 데이터를 처리하는 시스템을 일괄 처리를 담당하는 영역 (일괄 처리 계층) 과, 실시간 처리를 담당하는 영역 (속도 계층) 으로 나눈후 다음과 같이 운영합니다.</p>
<ol>
<li>새로운 데이터는 일관 처리 계층과 속도 계층 모두에 전달</li>
<li>일괄 처리 계층은 원본 데이터를 저장하고 일정 주기마다 한번씩 일괄적으로 가공해서 Batch View (결과 데이터) 를 생성</li>
<li>속도 계층은 들어오는 데이터를 즉시 또는 매우 짧은 주기로 처리해서 실시간 뷰를 생성</li>
<li>서빙 계층은 실시간 뷰와 배치 뷰의 결과를 적절히 조합해서 사용자에게 데이터를 전달</li>
</ol>
<p>정리하면, <strong>일괄 처리 작업을 통해 데이터를 처리하되 아직 배치 처리가 수행되지 않은 부분은 실시간 처리를 통해 보완</strong>한다는 개념입니다.</p>
<h3 id="1-2-스파크-설치"><a href="#1-2-스파크-설치" class="headerlink" title="1.2 스파크 설치"></a>1.2 스파크 설치</h3><h4 id="1-2-1-스파크-실행-모드의-이해"><a href="#1-2-1-스파크-실행-모드의-이해" class="headerlink" title="1.2.1 스파크 실행 모드의 이해"></a>1.2.1 스파크 실행 모드의 이해</h4><p>대부분의 빅데이터 소프트웨어들이 클러스터 환경에서 동작합니다. 클러스터란 <strong>여러대의 컴퓨터가 하나의 그룹을 형성해서 마치하나의 컴퓨터인 것처럼 동작하는 것</strong>을 의미합니다. 이처럼 여러 대의 서버가 마치 한 대의 서버처럼 동작해야하기 때문에 CPU 나 메모리, 디스크 등의 자원관리가 쉽지 않습니다. </p>
<p>스파크나 하둡과 같이 클러스터 환경에서 동작하는 대부분의 프레임워크는 실행 모드라는 개념을 가지고 있습니다. 즉, 개발 및 테스트를 위해서는 1대의 단독 서버 혹은 개인 PC 에서 애플리케이션을 실행하고, 실 서비스에서는 여러 서버로 구성된 클러스터 환경에서 동일한 어플리케이션을 실행할 수 있는 것입니다.</p>
<h4 id="1-2-5-스파크-쉘"><a href="#1-2-5-스파크-쉘" class="headerlink" title="1.2.5 스파크 쉘"></a>1.2.5 스파크 쉘</h4><p>스파크 어플리케이션을 실행하기 위해서는 <strong>메인 함수를 가진 어플리케이션을 작성하고, 스파크에서 제공하는 spark-submit 스크립트를 이용해서 실행</strong>합니다. </p>
<p>스파크는 프로그램을 작성해서 실행하는 방법 외에도, 인터렉티브 방식으로 프로그램을 작성할 수 있는 스파크 쉘도 제공합니다. 스파크 쉘은 사용하는 언어에 따라 스칼라, 파이썬, R 버젼으로 나눌 수 있습니다. </p>
<h4 id="1-2-6-실행-옵션"><a href="#1-2-6-실행-옵션" class="headerlink" title="1.2.6 실행 옵션"></a>1.2.6 실행 옵션</h4><p>스파크 쉘은 실행과 관련된 다양한 옵션을 제공합니다. “–help” 옵션으로 확인할 수 있습니다.</p>
<p>“–master” 옵션은 스파크가 사용할 <strong>클러스터의 마스터 정보를 지정하는 옵션</strong>입니다. 사용하는 클러스터 마스터 ( 혹은 메니저) 정보를 지정하는 옵션입니다.</p>
<p>만약 클러스터가 아닌 단일서버에서 동작시킬 경우에 “local” 이라고 입력합니다. 이 경우, 스파크 잡은 하나의 서버에서 하나의 스레드만 이용해서 동작합니다. 따라사 여러개의 스레들르 이용하려면 “local[2]” 처럼 지정합니다. 스레드 두 개를 사용한다는 의미입니다. “local[*]” 는 사용 가능한 모든 스레드를 사용합니다.</p>
<h3 id="1-4-예제-프로젝트-설정"><a href="#1-4-예제-프로젝트-설정" class="headerlink" title="1.4 예제 프로젝트 설정"></a>1.4 예제 프로젝트 설정</h3><h4 id="1-4-1-WordCount-예제-실행"><a href="#1-4-1-WordCount-예제-실행" class="headerlink" title="1.4.1 WordCount 예제 실행"></a>1.4.1 WordCount 예제 실행</h4><p>스파크 어플리케이션을 프로그래밍 하는 방법은, </p>
<ol>
<li>SparkContext 생성</li>
<li>입력 소스로부터 RDD 생성</li>
<li>RDD 처리</li>
<li>결과 파일 처리</li>
<li>SparkContext 종료</li>
</ol>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><p class="title is-size-4 is-block line-height-inherit">Junhee Ko</p><p class="is-size-6 is-block">Always Learning</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Incheon</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">310</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">15</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/kojunhee" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/kojunhee"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://www.facebook.com/kojunheee/"><i class="fab fa-facebook"></i></a></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">180</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/big-data/"><span class="level-start"><span class="level-item">Big Data</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/boost-course/"><span class="level-start"><span class="level-item">Boost Course</span></span><span class="level-end"><span class="level-item tag">15</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/git/"><span class="level-start"><span class="level-item">Git</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/jpa/"><span class="level-start"><span class="level-item">JPA</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/kafka/"><span class="level-start"><span class="level-item">Kafka</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/oop/"><span class="level-start"><span class="level-item">OOP</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/os/"><span class="level-start"><span class="level-item">OS</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/redis/"><span class="level-start"><span class="level-item">Redis</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/spark/"><span class="level-start"><span class="level-item">Spark</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/spring/"><span class="level-start"><span class="level-item">Spring</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/tdd/"><span class="level-start"><span class="level-item">TDD</span></span><span class="level-end"><span class="level-item tag">32</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/test-code/"><span class="level-start"><span class="level-item">Test Code</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/etc/"><span class="level-start"><span class="level-item">etc</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><h3 class="menu-label">Recent</h3><article class="media"><div class="media-content size-small"><p><time dateTime="2020-08-30T15:00:00.000Z">2020-08-31</time></p><p class="title is-6"><a class="link-muted" href="/2020/08/31/collection-framework-set/">Collection Framework - HashSet</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/java/">Java</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-08-29T15:00:00.000Z">2020-08-30</time></p><p class="title is-6"><a class="link-muted" href="/2020/08/30/collection-framework-iterator/">Collection Framework - Iterable</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/java/">Java</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-08-28T15:00:00.000Z">2020-08-29</time></p><p class="title is-6"><a class="link-muted" href="/2020/08/29/collection-framework-list/">Collection Framework - List</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/java/">Java</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-08-22T15:00:00.000Z">2020-08-23</time></p><p class="title is-6"><a class="link-muted" href="/2020/08/23/shallow-deep-copy/">Shallow Copy, Deep Copy</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/java/">Java</a></p></div></article><article class="media"><div class="media-content size-small"><p><time dateTime="2020-08-13T15:00:00.000Z">2020-08-14</time></p><p class="title is-6"><a class="link-muted" href="/2020/08/14/redis/">Redis 운영 관리</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/redis/">Redis</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-6880109808178384" data-ad-slot="3347750970" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Always Learning" height="28"></a><p class="size-small"><span>&copy; 2020 junhee.ko</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'https://kojunhee.github.io',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to Top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>